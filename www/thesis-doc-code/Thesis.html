<!doctype html>
<meta charset=utf-8>
<link rel="stylesheet" href="./css/highlight-syntax.min.css">
<script src="./js/highlight-syntax.min.js"></script>
<script src="./js/highlight-python.min.js"></script>
<style>
    body {
        width: 100%;
        height: 100%;
        margin: 0;
        padding: 0;
        background-color: #fff;
        font-size: 12pt;
        font-family: "Times New Roman", serif
    }

    * {
        box-sizing: border-box;
        -moz-box-sizing: border-box;
    }

    .page {
        width: 210mm;
        min-height: 297mm;
        padding: 20mm;
        margin: 10mm auto;
        border: 1px #fff solid;
        border-radius: 5px;
        background: #fff;
        box-shadow: 0 0 5px rgba(0, 0, 0, .1)
    }

    .subpage {
        padding: 1cm;
        border: 5px #fff solid;
        height: 257mm;
        outline: 2cm #fff solid;
        position: relative;
    }

    .toc_chapter {
        text-align: center;
    }

    .toc_section,
    .toc_subsection {
        text-align: left;
    }

    p,
    ol,
    li {
        text-align: left;
        line-height: 150%;
        text-align: justify;
    }

    a {
        color: #33f;
        text-decoration: none;
    }

    code {
        text-align: left;
        font-size: 10pt;
    }

    @page {
        size: A4;
        margin: 0
    }

    @media print {

        body,
        html {
            width: 210mm;
            height: 297mm
        }

        .page {
            margin: 0;
            border: initial;
            border-radius: initial;
            width: initial;
            min-height: initial;
            box-shadow: initial;
            background: initial;
            page-break-after: always
        }
    }
</style>
<script src=./js/jquery.min.js></script>
<script src=./js/bibtex.js></script>

<textarea id="bibtex_input" style="display:none;">
    @online{Medium2018,
        author = "Elvis Saravia",
        title = "Copyright 2020 A Medium Corporation. {State of the art Multimodal Sentiment Classification in Videos}",
        url  = "https://medium.com/dair-ai/state-of-the-art-multimodal-sentiment-classification-in-videos-1daa8a481c5a",
        addendum = "Accessed on: May 17, 2020.",
    }
    @article{Pouyanfar2017,
        author = {Pouyanfar, Samira and Chen, Shu-Ching},
        year = {2017},
        month = {03},
        pages = {85-109},
        title = {Automatic Video Event Detection for Imbalance Data Using Enhanced Ensemble Deep Learning},
        volume = {11},
        journal = {International Journal of Semantic Computing},
        doi = {10.1142/S1793351X17400050}
    }

    @INPROCEEDINGS{Yang2015,
        author={Y. {Yang} and S. {Chen}},
        booktitle={2015 IEEE International Conference on Information Reuse and Integration}, 
        title={Ensemble Learning from Imbalanced Data Set for Video Event Detection}, 
        year={2015},
        volume={},
        number={},
        pages={82-89},
    }
    @inproceedings{Wan2014,
        author = {W., Ji and Wang, Dayong and Hoi, Steven Chu Hong and Wu, Pengcheng and Zhu, Jianke and Zhang, Yongdong and Li, Jintao},
        title = {Deep Learning for Content-Based Image Retrieval: A Comprehensive Study},
        year = {2014},
        isbn = {9781450330633},
        publisher = {Association for Computing Machinery},
        address = {New York, NY, USA},
        url = {https://doi.org/10.1145/2647868.2654968},
        doi = {10.1145/2647868.2654968},
        booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
        pages = {157–166},
        numpages = {10},
        keywords = {feature representation, content-based image retrieval, convolutional neural networks, deep learning},
        location = {Orlando, Florida, USA},
        series = {MM ’14}
    }
    @misc{Lin2013,
        title={Network In Network},
        author={Min Lin and Qiang Chen and Shuicheng Yan},
        year={2013},
        eprint={1312.4400},
        archivePrefix={arXiv},
        primaryClass={cs.NE}
    }
    @ARTICLE{Chen2009,
        author={X. {Chen} and C. {Zhang} and S. {Chen} and S. {Rubin}},
        journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
        title={A Human-Centered Multiple Instance Learning Framework for Semantic Video Retrieval}, 
        year={2009},
        volume={39},
        number={2},
        pages={228-233},
    }

    @INPROCEEDINGS{Kolekar2008,
        author={M. H. {Kolekar} and K. {Palaniappan} and S. {Sengupta}},
        booktitle={2008 Sixth Indian Conference on Computer Vision, Graphics   Image Processing}, 
        title={Semantic Event Detection and Classification in Cricket Video Sequence}, 
        year={2008},
        volume={},
        number={},
        pages={382-389},
    }

    @INPROCEEDINGS{Lin2007,
        author={L. {Lin} and G. {Ravitz} and M. {Shyu} and S. {Chen}},
        booktitle={2007 IEEE International Conference on Multimedia and Expo}, 
        title={Video Semantic Concept Discovery using Multimodal-Based Association Classification}, 
        year={2007},
        volume={},
        number={},
        pages={859-862},
    }

    @INPROCEEDINGS{Chen2007,
        author={M. {Chen} and C. {Zhang} and S. {Chen}},
        booktitle={International Conference on Semantic Computing (ICSC 2007)}, 
        title={Semantic Event Extraction Using Neural Network Ensembles}, 
        year={2007},
        volume={},
        number={},
        pages={575-580},
    }


    @ARTICLE{Lecun1998,
        author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
        journal={Proceedings of the IEEE}, 
        title={Gradient-based learning applied to document recognition}, 
        year={1998},
        volume={86},
        number={11},
        pages={2278-2324},
    }

    @misc{kkroening, title={kkroening/ffmpeg-python},
        url={https://github.com/kkroening/ffmpeg-python/blob/master/examples/README.md}, 
        journal={GitHub.com}, 
        author={Kkroening}, 
        year={2020}, 
        month={May}
    }
    @misc{opencv4nodejs_2020, 
        title={opencv4nodejs}, 
        url={https://github.com/justadudewhohacks/opencv4nodejs}, 
        journal={GitHub}, 
        author={opencv4nodejs}, 
        year={2020}, 
        month={May}
    }

    @misc{muhler2017, 
        title={Node.js meets OpenCV's Deep Neural Networks - Fun with Tensorflow and Caffe}, 
        url={https://medium.com/@muehler.v/node-js-meets-opencvs-deep-neural-networks-fun-with-tensorflow-and-caffe-ff8d52a0f072}, 
        journal={Medium}, 
        publisher={Medium}, 
        author={Mühler, Vincent}, 
        year={2017}, 
        month={Dec}
    }
    @misc{uberi_2019, 
        title={Uberi/speech_recognition}, 
        url={https://github.com/Uberi/speech_recognition}, 
        journal={GitHub}, 
        author={Uberi}, 
        year={2019}, 
        month={Jul}
    }
    @misc{tensorflow2015-whitepaper,
        title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
        url={https://www.tensorflow.org/},
        note={Software available from tensorflow.org},
        author={
            Martin Abadi and
            Ashish Agarwal and
            Paul Barham and
            Eugene Brevdo and
            Zhifeng Chen and
            Craig Citro and
            Greg S. Corrado and
            Andy Davis and
            Jeffrey Dean and
            Matthieu Devin and
            Sanjay Ghemawat and
            Ian Goodfellow and
            Andrew Harp and
            Geoffrey Irving and
            Michael Isard and
            Yangqing Jia and
            Rafal Jozefowicz and
            Lukasz Kaiser and
            Manjunath Kudlur and
            Josh Levenberg and
            Dandelion Man\'{e} and
            Rajat Monga and
            Sherry Moore and
            Derek Murray and
            Chris Olah and
            Mike Schuster and
            Jonathon Shlens and
            Benoit Steiner and
            Ilya Sutskever and
            Kunal Talwar and
            Paul Tucker and
            Vincent Vanhoucke and
            Vijay Vasudevan and
            Fernanda Viegas and
            Oriol Vinyals and
            Pete Warden and
            Martin Wattenberg and
            Martin Wicke and
            Yuan Yu and
            Xiaoqiang Zheng},
          year={2015},
        }
    @misc{scipy,
         title={SciPy}, 
         author={SciPy}, 
         url={https://docs.scipy.org/doc/scipy/reference/index.html}, 
         journal={SciPy}, 
         year={2020}, 
         month={May}
    }
    @misc{bambocher_2018, 
        title={bambocher/pocketsphinx-python}, 
        url={https://github.com/bambocher/pocketsphinx-python}, 
        journal={GitHub}, 
        author={Bambocher}, 
        year={2018}, 
        month={Jun}
    }
    @misc{spaCy2020, 
        author={spaCy}, 
        title={Industrial-strength Natural Language Processing in Python}, 
        url={https://spacy.io/}, 
        journal={Natural Language Processing in Python},
        year={2020}, 
        month={May}
    }
    @misc{pytorch_2020, 
        title={pytorch/pytorch}, 
        url={https://github.com/pytorch/pytorch}, 
        journal={GitHub}, 
        author={Pytorch}, 
        year={2020}, 
        month={May}
    }
    @misc{desagulier, 
        title={Word embeddings: the (very) basics}, 
        url={https://corpling.hypotheses.org/495}, 
        journal={Around the word}, 
        author={Desagulier, Guillaume},
        year={2020}, 
        month={May}
    }  
    @misc{ wiki:xxx,
        author = "{Wikipedia}",
        title = "Fast Fourier transform --- {Wikipedia}{,} The Free Encyclopedia",
        year = "2020",
        url = "https://en.wikipedia.org/w/index.php?title=Fast_Fourier_transform&oldid=959123323",
        year={2020}, 
        month={May}
      }
    @misc{intel, 
        author={Intel},
        title={Inception V3 Deep Convolutional Architecture For Classifying Acute...}, 
        url={https://software.intel.com/content/www/us/en/develop/articles/inception-v3-deep-convolutional-architecture-for-classifying-acute-myeloidlymphoblastic.html}, 
        journal={Intel},
        year={2020}, 
        month={May}
    }
    @misc{shrimali_2019, 
        title={Image Classification using Pre-trained models}, 
        url={https://www.learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/}, 
        journal={Learn OpenCV}, 
        author={Shrimali, Vishwesh}, 
        year={2019}, 
        month={Jun}
    }
    @misc{youtube2020,
        title={Screen Recording of CrazedCoding.com Audio and Video Processing Server},
        year={2019}, 
        month={Jun},
        organization={Youtube},
        author={Crazed Coding},
        url={https://www.youtube.com/watch?v=Z1HLFvvoKEk}
    }

    @misc{youtube2012,
        title={Turret Opera A Cappella Portal 2},
        year={2012}, 
        month={Dec},
        organization={Youtube},
        author={Triangleman},
        url={https://www.youtube.com/watch?v=q-vhf6GfT14}
    }
</textarea>
<div id=100_mm_ruler style=height:100mm;display:none></div>
<div class=page>
    <div class=subpage>
        <br>
        <p style=text-align:center;font-size:26pt><strong>A Novel Multimodal Model for<br>Event Detection in
                Videos</strong></p>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <p style=text-align:center;font-size:18pt>Ian Logan Wesson
            <br>(安龙)</p>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <p style=text-align:center;font-size:18pt>June 2020
    </div>
</div>
<div class=page>
    <div class=subpage>
        <br>
        <p style=text-align:center;font-size:26pt><strong>A Novel Multimodal Model for<br>Event Detection in
                Videos</strong></p>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <div
            style="width:49%;font-size:14pt; margin-left: 7.5%; display: inline-block; white-space: nowrap;text-overflow:ellipsis;overflow: hidden;">
            <p style="width: 49%; margin: 0px;margin-bottom: 10pt;">
                Candidate.............................................................................</p>
            <p style="margin: 0px;margin-bottom: 10pt;">School of
                Study.............................................................................</p>
            <p style="margin: 0px;margin-bottom: 10pt;">Student’s
                Number.............................................................................</p>
            <p style="margin: 0px;margin-bottom: 10pt;">
                Supervisor.............................................................................</p>
            <p style="margin: 0px;margin-bottom: 10pt;">Chair of Defense
                Committee.............................................................................</p>
            <p style="margin: 0px;margin-bottom: 10pt;">Degree
                Applied.............................................................................</p>
            <p style="margin: 0px;margin-bottom: 10pt;">
                Major.............................................................................</p>
            <p style="margin: 0px;margin-bottom: 10pt;">Date of
                Defense.............................................................................</p>
        </div>
        <div
            style="width:41%;font-size:14pt; display: inline-block; white-space: nowrap;text-overflow:ellipsis;overflow: hidden;">
            <p style="margin: 0px;margin-bottom: 10pt">Ian Logan Wesson 安龙 </p>
            <p style="margin: 0px;margin-bottom: 10pt">School of Computer Science</p>
            <p style="margin: 0px;margin-bottom: 10pt">3820181125 </p>
            <p style="margin: 0px;margin-bottom: 10pt">张华平 Hua Ping Zhang</p>
            <p style="margin: 0px;margin-bottom: 10pt">&nbsp;</p>
            <p style="margin: 0px;margin-bottom: 10pt">Master of Computer
                Science </p>
            <p style="margin: 0px;margin-bottom: 10pt">Computer Science and
                Technology </p>
            <p style="margin: 0px;margin-bottom: 10pt">June 2020 </p>
        </div>
    </div>
</div>
<div class=page>
    <div class=subpage>
        <p style=text-align:center;margin-bottom:0;font-size:16pt><strong>Declaration of
                Originality</strong></p>
        <p style=margin-bottom:0;text-align:justify>
            <br></p>
        <p style=margin-bottom:0;text-align:justify>I hereby declare that this thesis is
            the result of an independent research that I have made under the supervision of my supervisor.
            To the best of my knowledge and belief, it does not contain any other published or unpublished
            research work of others, or any material which has been accepted for the award of another degree
            or diploma at Beijing Institute of Technology or other educational institutions, except where
            due acknowledgment has been made in the text. Whoever has contributed to this study is
            explicitly identified and appreciated in the Acknowledgements of the thesis.</p>
        <p style=margin-bottom:0;text-align:justify>
            <br></p>
        <p style=margin-bottom:0;line-height:125%;text-align:justify>Signature of
            Author: <u>      </u></p>
        <p style=margin-bottom:0;line-height:125%;text-align:justify>Date: <u>      </u></p>
        <p style=margin-bottom:0;text-align:justify>
            <br></p>
        <p style=margin-bottom:0;text-align:justify>
            <br></p>
        <p style=margin-bottom:0;text-align:justify>
            <br></p>
        <p style=text-align:center;margin-bottom:0;font-size:16pt>
            <strong>Authorization Statement</strong>
            <p style=margin-bottom:0;text-align:justify>
                <br></p>
            <p style=margin-bottom:0;text-align:justify>I
                fully understand the regulations of Beijing Institute of
                Technology regarding the preservation and use of the degree
                thesis. BIT is granted the right to (1) preserve the
                original thesis and the copies of the <em class=western><span
                        style=font-style:normal>thesis</span></em>, as well
                as submit them to relevant authorities; (2) reproduce and
                preserve the thesis by means of photocopy, reduction
                printing and any other means; (3) keep the thesis for
                reference and borrowing; (4) reproduce, give and exchange
                the thesis for the purpose of academic exchange; (5) publish
                the <em class=western><span style=font-style:normal>thesis
                        wholly and partially</span></em>. (The regulations
                comes into force for confidential thesis only after
                declassification)</p>
            <p style=margin-bottom:0;text-align:justify>
                <br></p>
            <p style=margin-bottom:0;text-align:justify>
                Student’s signature： <u>      </u>   Date:
                 <u>      </u></p>
            <p style=margin-bottom:0;text-align:justify>
                Supervisor’s signature： <u>      </u>   Date:
                 <u>      </u></p>
    </div>
</div>
<toc_frontmatter_entry class=toc_frontmatter_entry id=Acknowledgments>
    <p>   I personally, and this research, have been helped along by quite a
        few different people, all contributing in their own way.
        I would like to thank: (1) my family for their relentless
        support for my education, (2) my professors at BIT for their
        invaluable academic guidance and assistance adapting to a new
        country, and (3) the People's Republic of China for their awesome
        scholarship program, accomidating nature, and help
        exploring a formerly cryptic side of world.
    </p>
    <p>   I am especially grateful for all the intelligent, hard-working, and kind
        friends I have met during my studies
        at BIT; the pages of this thesis could be entirely filled with this
        endless list of names, but I used WeChat for storing that list
        instead.
    </p>
    <p>-Ian Wesson (安龙)
    </p>
</toc_frontmatter_entry>
<toc_frontmatter_entry class=toc_frontmatter_entry id=Abstract>
    <p>   Querying videos for many types of information, from cats and dogs, to instances of
        crime, abuse, violence or terrorism, in a way that returns a frame-by-frame analysis is becoming more and
        more important with the rapid expansion of online videos, especially for social media sites. Although there
        is some research into video classification, event detection in video analysis is still largely
        underdeveloped. For starters, even readily available open- source research into video event detection does
        not use a multimodal approach. Moreover, natural language processing and linguistic modalities are often not
        even used as a
        way of detecting events in any video analysis software. Last but not least, all frame-by-frame solutions to
        video analysis are proprietary or closed-source.</p>
    <p>   Compared to other modern video analysis tools and techniques, the proposed
        framework: (1) will implement a novel multimodal model for detecting events; (2) will utilize existing
        natural language processing software as an intermediate processing layer; and (3) will not be
        proprietary or closed source, but instead be built from entirely open-source software and hosted on a
        publicly accessible source code revision control platform (GitHub). The proposed framework has changed
        since there has been time to develop it, as many different tools and methodologies were briefly tested
        as a robust prototype system evolved over the duration of the research.</p>
    <p>   The main innovation of this research is a client-server system which combines
        frame image classification probabilities and classes with the their class word
        embeddings. This enables the desired ability of the end user to create complex visual queries
        of video frames via the positive and negative query strings quickly and efficiently, with ever
        improving accuracy and precision with each refinement of a query.</p>
    <p>   The main contribution of this research is final open-source project. The academic and
        open-source coding communities will hopefully benifit from a good example on how to
        implement this type of research. The final open-source project could be used as a learning
        tool, or even example code, for creating more effective neural network based architectures for
        video content monitoring and browsing in the future.</p>
    <div style=position:relative><span
            style="font-style:normal;font-weight:700;font-size:12pt;font-family:Times New Roman;color:#000">Key
            Words: </span><span
            style="font-style:normal;font-weight:400;font-size:12pt;font-family:Times New Roman;color:#000">Multimodal;
            Neural Network; Machine Learning; Computer Vision; Natural Language Processing; Audio Signal
            Processing; Time Series Prediction;</span></div>
</toc_frontmatter_entry>
<toc_frontmatter_entry id="Table of Contents" class="toc_frontmatter_entry table_of_contents">
    <div id=toc_list style="list-style-type:none;"></div>
</toc_frontmatter_entry>
<toc_chapter class=toc_chapter id=Introduction>

    <toc_section class=toc_section id="Background">
    </toc_section>
    <p>   The proposed framework was designed around a novel multimodal model which would
        process the output of the following open-source NodeJS software solutions: automatic speech recognition
        (using the deepspeech npm library), object detection (using the opencv4nodejs or tensorflowjs npmlibraries),
        and natural language processing (using the word2vector npm library).</p>
    <p>   The main objective of the work proposed was finding the best combinations of
        libraries, programming tools, and even operating systems, suitable for creating an efficient and
        innovative neural network audio/video applications research and development environment. During this extensive
        search, different API’s, such as OpenCV, PyTorch, and TensorFlow, in various languages, including NPM/NodeJS,
        JavaScript, and Python, were tested on the Windows 10, Ubuntu 19.04, and Linux Mint 19.3 operating
        systems.</p>
    <p>   For our particular hardware setup, the most suitable OS, library, and language
        we found for developing machine learning software ended up being as follows:
        Linux Mint with Pytorch and TensorFlow/Keras in Python 3.6.9.</p>

    <p>   You can find an
        online copy of the final project code on GitHub. The project link you should refer to when
        reading this document is a link is to a specific branch named ”thesis” of a GitHub repository
        (which will host future developments on the ”master” branch):</p>
    <p style="font-family: 'Courier New', Courier, monospace;font-size: 11pt;">
        https://github.com/CrazedCoding/CrazedCoding.com/tree/thesis</p>

    <p>   One of the accomplishments of this research includes a multimodal
        neural network model based system that allows users to process videos frame-by-frame for
        images with content of varying degrees of similarity to query string parameters. The research
        on this topic covered in
        <toc_reference name="Event Detection for Videos"></toc_reference> through
        <toc_reference name="Computer Vision"></toc_reference>, and the experiments for this are
        covered in <toc_reference name="Novel Multimodal Model Architecture for Event Detection"></toc_reference>
    </p>
    <p>   The second accomplishment of this
        research was the creation of a command-line tool which enabled the user to train a
        multi-layer neural network to learn and predict audio samples. The research on this topic
        covered in
        <toc_reference name="Audio Signal Analysis"></toc_reference> and the experiments are in
        <toc_reference name="Audio Signal Time Series Prediction"></toc_reference>.
    </p>
    <page_break></page_break>
    <toc_section class=toc_section id="Previous Work">
    </toc_section>
    <p>   Of the many other various audio/video machine learning approaches that exist today, several are
        discussed in detail in <toc_reference name="Research Related to Video Event Detection"></toc_reference>:</p>
    <ol>
        <li>An ensemble deep learning framework than analyzes soccer videos is discussed in
            <toc_reference name="Deep Learning"></toc_reference>
        </li>
        <li>A multimodal deep learning framework for classifying videos is discussed in
            <toc_reference name="Multimodal Models"></toc_reference>
        </li>
        <li>Classification of video sequences in cricket videos is discussed in
            <toc_reference name="Event Detection for Videos"></toc_reference>
        </li>
        <li>The beginning of <toc_reference name="Computer Vision"></toc_reference>
            has <toc_reference name="code1"></toc_reference> and <toc_reference name="code2"></toc_reference>
            which are examples on how to load video for the following two
            subsections:
            <ul>
                <li>
                    <toc_reference name="Image Classification"></toc_reference> provides <toc_reference name="code3">
                    </toc_reference>, an example of modern image classification.
                </li>
                <li>
                    <toc_reference name="Object Detection"></toc_reference> provides <toc_reference name="code4">
                    </toc_reference>, an example of modern object detection.
                </li>
            </ul>
        </li>
        <li>The beginning of <toc_reference name="Audio Signal Analysis"></toc_reference>
            has <toc_reference name="code5"></toc_reference>,
            which is an example on how to load audio for the following subsections:
            <ul>
                <li>
                    <toc_reference name="Fast Fourier Transform"></toc_reference> provides
                    an examples of what the fourier transform is and how to use it.
                </li>
                <li>
                    <toc_reference name="Automatic Speech Recognition"></toc_reference> provides <toc_reference
                        name="code6"></toc_reference>, an example of modern automatic speech recognition.
                </li>
            </ul>
        </li>
        <li>The beginning of <toc_reference name="Natural Language Processing"></toc_reference>
            discusses natural language processing.
            <ul>
                <li>
                    <toc_reference name="Word Embeddings"></toc_reference> provides <toc_reference name="code7">
                    </toc_reference>, an example of modern word2vec usage, as well as visualizations
                    of word embeddings and cosine similarity operations.
                </li>
            </ul>
        </li>
    </ol>
    <p>   YouTube's
        approach for querying content is not discussed, as it appears to be mainly centered around the tags, comments,
        titles, descriptions, and other meta-data associated with each video. Additionaly, YouTube does not appear to
        readily allow the end user to: (1) create a frame-by-frame query of a uploaded video's visual content, or (2) to
        create a speech-to-text/audio-classification query of a videos audio streams.</p>
    <p>   These types of advanced query features, if they exist at all, might be reserved for moderators of YouTube,
        and do not appear to be in
        any public and open-source software repositories on GitHub. Additionally, other major video media hosting
        sites such as Vimeo, PutLocker, and Youku do not appear
        to provide the end user with these features. </p>
    <page_break></page_break>
    <toc_section class=toc_section id="Structure">
    </toc_section>
    <p>   The original design for our neural network experiments is shown in <toc_reference name="figure3">
        </toc_reference> below.
        It was designed to test three hypotheses to three questions which are listed in <toc_reference name="table1">
        </toc_reference> on the following page.</p>
    <toc_figure id="figure3" description="Overview of the originally proposed framework.">
        <img src=./imgs/figure3.png style=max-width:90%>
    </toc_figure>
    <page_break></page_break>

    <p>   The design for our neural network experiment changed drastically over the duration
        of our research (shown later in <toc_reference name="figure4"></toc_reference>). The three
        questions and their corresponding hypotheses to be tested are listed in <toc_reference name="table1">
        </toc_reference> below, along with what our
        research and
        experiments indicated about the answers.</p>
    <toc_table id="table1" description="Hyptheses, questions, and answers of the research project.">
        <table style="border-collapse:collapse;border-spacing:0" class="tg">
            <thead>
                <tr>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Questions</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Hypotheses</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Answers</th>
                </tr>
            </thead>
            <tbody style="text-align:left;">
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Is it possible to use entirely open-source
                            technology to build the framework of a video based event detection algorithm?</span><br>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Yes, because there are many open source
                            projects
                            for the basic requirements of such a framework.</span></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Moderately supported by research described in
                            sections <toc_reference name="Automatic Speech Recognition"></toc_reference>. Audio modality
                            not supported
                            by experiments at all (due to noise).</span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Is there a multimodal model for fast and
                            accurate
                            event detection on video media?</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Yes. This hypothesis is based on the fact
                            current
                            research is capable of detecting anomalous and suspicious activity in images.</span><br>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Highly supported by all of the reserch and in
                            this chapter's research of <toc_reference name="Research Related to Video Event Detection">
                            </toc_reference>,
                            and the experiments described in <toc_reference
                                name="Novel Multimodal Model Architecture for Event Detection"></toc_reference>,
                            particularly the ones described in <toc_reference
                                name="How-To Usage Guide of the Multimodal System"></toc_reference>
                            .</span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Can multimodal models improve the performance
                            of
                            content moderators?</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Yes, because video content can be more
                            accurately
                            and precisely analyzed for events.</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">This is highly supported by the video
                            processing research and experiments
                            , and the moderator’s point of view is demonstrated in
                            <toc_reference name="How-To Usage Guide of the Multimodal System"></toc_reference>
                        </span><br></td>
                </tr>
            </tbody>
        </table>
    </toc_table>

    <p>   Most of our hypotheses were strongly supported by the results the experiments
        (described in <toc_reference name="Novel Multimodal Model Architecture for Event Detection"></toc_reference>
        and <toc_reference name="Audio Signal Time Series Prediction"></toc_reference>),
        with one notable exception to the first hypothesis: our
        research (all described in <toc_reference name="Research Related to Video Event Detection"></toc_reference>)
        indicated that even though there are many open-source software solutions for automatic
        speech recognition (ASR), virtually all of them suffer from severe inaccuracy when
        transcribing audio without proper recording conditions or denoising techniques. The decision
        was made to abandon ASR as a solution for creating one of the modes for the multimodal network.</p>

    <p>   The pitfall of losing the preferred choice of auditory mode (spoken words from audio
        signals transcribed as text) made the process of creating a multimodal system difficult because
        our only other preferred choices of modes were going to be the visual and spacial modes of a
        video stream.</p>
    <p>   In addition to this pitfall, there was another: the
        variety
        of information available as output from pretrained object detection algorithms (which we
        planned to use for the visual and spacial modes of our multimodal system) was very limited:
        most pretrained object detection models for desktop environments can produce a maximum
        of around 10 visual classes (the types of the objects detected), which, even in combination
        with spacial output mode of object detection algorithms (the positions of the objects detected),
        could not fulfil the requirement of producing a search space out of the input videos (which
        had a variety of types of content) accurate, precise, or even organized enough to query for
        generalized user search string parameters.</p>
    <p>   The two aforementioned pitfalls meant that we could not use ASR or object detection
        to create any of the input modes for our multimodal system as we had planned to initially.
        To meet time requirements, image classification models would instead be used for the visual
        mode of the system, which could output around 1000 different output classes per image.
        Additionally, natural language processing (NLP) word embeddings would be used to create
        a linguistic mode of input for our multimodal system.</p>
    <p>   Even though using these two modes of input (visual and linguistic) for our system
        would
        meet the initial requirement of creating a multimodal system, doing just this would have
        meant the final product of our video-processing research would entirely ignore the audio. The
        research project was initially designed to include audio signal analysis (in the form of ASR)
        to create a complete understanding of applying multimodal neural networks in the context
        visual and auditory analysis of videos, so following the conclusion of our research and experiments with
        video processing (covered in <toc_reference name="Computer Vision"></toc_reference> through
        <toc_reference name="Event Detection for Videos"></toc_reference>, and <toc_reference
            name="Novel Multimodal Model Architecture for Event Detection"></toc_reference>) we abruptly switched to
        audio signal analysis research and
        experiments
        (covered in <toc_reference name="Audio Signal Analysis"></toc_reference>
        and <toc_reference name="Audio Signal Time Series Prediction"></toc_reference>).
        The audio signal experiment comes in the form of a point-by-point signal
        prediction Pytorch Python 3.6.9 program.</p>
</toc_chapter>
<toc_chapter class=toc_chapter id="Research Related to Video Event Detection">
    <p>   The following sections provide a detailed overview of the state-of the art
        research into event detection in videos.</p>
    <toc_section class=toc_section id="Deep Learning">
    </toc_section>
    <p><a class="bibtex_reference" bibtexkey="Pouyanfar2017"></a> has the following to say about
        semantic event
        detection:</p>
    <p style=" margin-left:10%;">Recently, many researchers have tried to detect the most interesting
        events and concepts from
        videos (<a class="bibtex_reference" bibtexkey="Lin2007"></a>, <a class="bibtex_reference"
            bibtexkey="Chen2009"></a>). Criminal event detection from video and audio data, natural disaster retrieval
        from video
        data, and interesting event detection in a sport game are a few examples of video semantic event detection.</p>
    <p><a class="bibtex_reference" bibtexkey="Pouyanfar2017"></a> then goes into detail about it’
        s proposed ensemble deep learning framework:</p>
    <p style=" margin-left:10%;">Deep learning is not a new topic and has a long history in artificial
        intelligence (<a class="bibtex_reference" bibtexkey="Wan2014"></a>). Convolutional Neural Networks (CNNs) <a
            class="bibtex_reference" bibtexkey="Lecun1998"></a>, for instance, have improved traditional
        feedforward neural networks in 1990s, especially in image processing, by constraining the complexity of
        networks using local weight sharing topology. Traditional neural network techniques are difficult to
        interpret due to their black-box nature and they are also very prone to over-fitting <a class="bibtex_reference"
            bibtexkey="Yang2015"></a>. In contrast,
        new deep learning algorithms are more interpretable because of their strong local modeling. In addition,
        as new ideas, algorithms, and network architectures have been designed in the last few years, deep
        learning has shown significant advances mainly in image recognition and object detection.</p>
    <p style=" margin-left:10%;">As a single classifier may not be able to handle large datasets with
        multiple feature sources, ensemble algorithms have attracted lots of attention in the literature,
        which can be utilized to enhance the classification performance by taking advantages of multiple
        classifiers. A positive enhanced ensemble algorithm which handles imbalanced data in video event
        retrieval is presented <a class="bibtex_reference" bibtexkey="Chen2007"></a>. Their proposed framework combines
        a sampling-based method with a
        classifier fusion algorithm to enhance the detection of interesting events (minor classes) in an
        imbalanced sport video dataset. </p>
    <toc_figure id="figure1" description="A proposed ensemble deep learning framework.">
        <img src=./imgs/figure1.png style=max-width:100%>
    </toc_figure>
    <p style=" margin-left:10%;">An ensemble neural network is proposed in <a class="bibtex_reference"
            bibtexkey="Kolekar2008"></a>. Using a bootstrapped
        sampling approach along with a group of neural networks, the rare event issue is alleviated. The
        framework was also evaluated using a large set of soccer videos with the purpose of corner event
        detection.</p>
    <p>   <a class="bibtex_reference" bibtexkey="Pouyanfar2017"></a>’s method is divided into
        three main modules: (1) preprocessing, (2) deep feature
        extraction, and (3) classification including training, validation, and testing. <a class="bibtex_reference"
            bibtexkey="Pouyanfar2017"></a> concludes
        that: “the experimental results demonstrate the effectiveness of the proposed framework for
        video event detection.”
        detection.</p>
    <page_break></page_break>

    <toc_section class=toc_section id="Multimodal Models">
    </toc_section>
    <p>   According to <a class="bibtex_reference" bibtexkey="Medium2018"></a>, there are
        basically two frameworks for fusing modalities when designing neural networks:</p>
    <p style=" margin-left:10%"><b>Non-hierarchical</b> Frameworks where unimodal features are concatenated and fed into
        the various contextual LSTM networks proposed above (e.g., h-LSTM), and </p>
    <p style=" margin-left:10%"><b>Hierarchical Frameworks</b>
        where the difference here is that we don’ t concatenate unimodal features, we feed each
        unimodal feature into the LSTM network proposed above. Think of this framework as having
        some hierarchy. In the first level, unimodal features are fed individually to LSTM networks.
        The output of the first level are then concatenated and fed into another LSTM network (i.e.,
        second level).</p>
    <p>   For reference, the diagram demonstrating the multimodal model
        from <a class="bibtex_reference" bibtexkey="Medium2018"></a> is included in
        <toc_reference name="figure2"></toc_reference> on the following page. In testing the proposed model, <a
            class="bibtex_reference" bibtexkey="Medium2018"></a>
        gives the following findings: </p>
    <p style=" text-indent:10%">1. They observed
        that hierarchical model significantly outperform the non-hierarchical frameworks. </p>
    <p style=" text-indent:10%">2. They
        observed that sc-LSTM and bc-LSTM models perform the best out of the LSTM variants, including
        the uni-SVM model. These results helped to show the importance of considering contextual
        information when classifying utterances. </p>
    <p style=" text-indent:10%">3. In general, unimodal classifiers trained on
        textual information performed better compared to other individual modalities (results highlighted
        in blue). Combining the modalities tended to boost the performance, indicating that multimodal
        methods are feasible and effective. </p>
    <p style=" text-indent:10%">4. Individually, the visual modality caries
        more generalized information. Overall, fusing the modalities improved the model.</p>
    <toc_figure id="figure2" description="The overview of a multimodal framework.">
        <img src=./imgs/figure2.png style=max-width:75%>
    </toc_figure>

    <toc_section class=toc_section id="Event Detection for Videos">
    </toc_section>
    <p>   <a class="bibtex_reference" bibtexkey="Kolekar2008"></a> provides a good technique for
        classifying video sequences. It is included here as a potential solution for creating training
        mechanisms for deep learning models. </p>
    <p style=" margin-left:10%" ;>In this paper, video semantic analysis is formulated
        based on low-level image features and high-level knowledge. The
        sports domain semantic knowledge encoded in the hierarchical classification not only reduces
        the cost of processing data drastically, but also significantly increases the classifier accuracy.
        The hierarchical framework enables the use of simple features and organizes the set of features
        in a semantically meaningful way. The proposed hierarchical semantic framework for
        event classification can be readily generalized to other sports domains as well as other types
        of video.</p>
    <page_break></page_break>
    <toc_section class=toc_section id="Computer Vision">
    </toc_section>
    <p>   The following code segment from <a class="bibtex_reference" bibtexkey="kkroening"></a>
        demonstrates the most important process we found for
        loading videos into memory when using python. Our first approach was to use <a class="bibtex_reference"
            bibtexkey="pytorch_2020"></a>'s
        torchvision.io module, but
        we later opted for the <a class="bibtex_reference" bibtexkey="kkroening"></a>'s ffmpeg-python module to process
        a video files frame-by-frame
        using numpy because of it’s speed, versatility, and compatibility with Pytorch. You can find
        out more about how we used this functionality by referencing "video_processing.py” in the
        root folder of the GitHub branch accompanying this document.</p>

    <toc_code_segment id="code1" description="Overview of the ffmpeg/python video loading code.">
        <pre><code class="python hljs">
import ffmpeg
import subprocess
import sys
import numpy as np

process1 = (
    ffmpeg
    .input(in_filename)
    .output('pipe:', format='rawvideo', pix_fmt='rgb24')
    .run_async(pipe_stdout=True)
)
process2 = (
    ffmpeg
    .input('pipe:', format='rawvideo', pix_fmt='rgb24',\\
    s='{}x{}'.format(width, height))
    .output(out_filename, pix_fmt='yuv420p')
    .overwrite_output()
    .run_async(pipe_stdin=True)
)
while True:
    in_bytes = process1.stdout.read(width * height * 3)
    if not in_bytes:
        break
    in_frame = (
        np
        .frombuffer(in_bytes, np.uint8)
        .reshape([height, width, 3])
    )
    out_frame = in_frame * 0.3
    process2.stdin.write(
        frame
        .astype(np.uint8)
        .tobytes()
    )
process2.stdin.close()
process1.wait()
process2.wait()
        </code></pre>
    </toc_code_segment>
    <p>   Here is an additional code segment from <a class="bibtex_reference" bibtexkey="opencv4nodejs_2020"></a>
        we used for loading videos into memory
        when using NPM/NodeJS. OpenCV4NodeJS was our initial library of choice for this research endeavor,
        but that was long before we discovered that our GPU was incompatible with the libraries
        on our test system regardless of how we configured the operating system. So, it is only
        included here to accompany following object detection code, which we also later stopped
        using in favor of python-based image classification.
    </p>

    <toc_code_segment id="code2" description="Overview of the NPM/NodeJS video/webcam reading code.">
        <pre><code class="nodejs hljs">
// open capture from webcam
const devicePort = 0;
const wCap = new cv.VideoCapture(devicePort);

// open video capture
const vCap = new cv.VideoCapture('./path/video.mp4');

// read frames from capture
const frame = vCap.read();
vCap.readAsync((err, frame) => {
    ...
});

// loop through the capture
const delay = 10;
let done = false;
while (!done) {
    let frame = vCap.read();
    // loop back to start on end of stream reached
    if (frame.empty) {
    vCap.reset();
    frame = vCap.read();
    }

    // ...

    const key = cv.waitKey(delay);
    done = key !== 255;
}
        </code></pre>
    </toc_code_segment>
    <p>   This code can be configured to read from the webcam or a video file. It reads until either the
        stream is empty, or a key has been pressed. It provides the video frames of the webcam or file
        in the form of OpenCV4NodeJS Mat objects in real-time. It is great beginner-level code for using
        with the next two code segments, which show how to use the Mat objects for image classification and
        object detection.
    </p>
    <page_break></page_break>
    <toc_subsection class=toc_subsection id="Image Classification">
    </toc_subsection>
    <p>   The following code segment is from <a class="bibtex_reference" bibtexkey="muhler2017"></a>
        and demonstrates how to use the Mat objects (which we previously showed how to load
        from video files and the webcam) as input to the tensorflow inception model.
    </p>

    <toc_code_segment id="code3" description="OpenCV4NodeJS image classification code.">
        <pre><code class="nodejs hljs">
// replace with path where you unzipped inception model
const inceptionModelPath = '../data/dnn/tf-inception'
const modelFile = path.resolve(inceptionModelPath, 
                        'tensorflow_inception_graph.pb');
const classNamesFile = path.resolve(inceptionModelPath, 
                        'imagenet_comp_graph_label_strings.txt');
// read classNames and store them in an array
const classNames = fs.readFileSync(classNamesFile)
                        .toString().split("\n");
// initialize tensorflow inception model from modelFile
const net = cv.readNetFromTensorflow(modelFile);
const classifyImg = (img) => {
    // inception model works with 224 x 224 images, so we resize
    // our input images and pad the image with white pixels to
    // make the images have the same width and height
    const maxImgDim = 224;
    const white = new cv.Vec(255, 255, 255);
    const imgResized = img.resizeToMax(maxImgDim)
        .padToSquare(white);
    // network accepts blobs as input
    const inputBlob = cv.blobFromImage(imgResized);
    net.setInput(inputBlob);
    // forward pass input through entire network, will return
    // classification result as 1xN Mat with confidences of classes
    const outputBlob = net.forward();
    // find all labels with a minimum confidence
    const minConfidence = 0.05;
    const locations =
        outputBlob
        .threshold(minConfidence, 1, cv.THRESH_BINARY)
        .convertTo(cv.CV_8U)
        .findNonZero();
    const result =
        locations.map(pt => ({
        confidence: parseInt(outputBlob.at(0, pt.x) * 100) / 100,
        className: classNames[pt.x]
        }))
        // sort result by confidence
        .sort((r0, r1) => r1.confidence - r0.confidence)
        .map(res => `${res.className} (${res.confidence})`);
    return result;
}
        </code></pre>
    </toc_code_segment>
    <page_break></page_break>
    <p>   The InceptionV3 model used in the previous code segment is best explained
        by <toc_reference name="figure2a"></toc_reference> (which is from <a class="bibtex_reference"
            bibtexkey="intel"></a>):
    </p>
    <toc_figure id="figure2a" description="InceptionV3 convolutional neural network and image classification model.">
        <img src="./imgs/InceptionV3.jpeg" style=max-width:90%>
    </toc_figure>
    <p>   Following the realization that there were a variety of pretrained
        image classification models available, we promply started research on comparisons
        between the different models. The best summararization of this research is in the
        form of <toc_reference name="figure2b"></toc_reference> (from <a class="bibtex_reference"
            bibtexkey="shrimali_2019"></a>):
    </p>
    <toc_figure id="figure2b" description="Comparison of pretrained image classification models.">
        <img src="./imgs/Comparison.png" style=max-width:90%>
    </toc_figure>
    <page_break></page_break>

    <toc_subsection class=toc_subsection id="Object Detection">
    </toc_subsection>
    <p>   The following code segment, also from <a class="bibtex_reference" bibtexkey="muhler2017"></a>,
        demonstrates how to use the Mat objects (which we previously showed how to load
        from video files and webcam as well as classify) as input to the
        Common Object in Context (COCO) Single Shot Multibox Detector
        (SSD).
    </p>

    <toc_code_segment id="code4" description="OpenCV4NodeJS object detection code.">
        <pre><code class="nodejs hljs">
// replace with path where you unzipped coco-SSD_300x300 model
const ssdcocoModelPath = '../data/dnn/coco-SSD_300x300'
const prototxt = path.resolve(ssdcocoModelPath, 'deploy.prototxt');
const modelFile = path.resolve(ssdcocoModelPath, 
    'VGG_coco_SSD_300x300_iter_400000.caffemodel');

// initialize ssdcoco model from prototxt and modelFile
const net = cv.readNetFromCaffe(prototxt, modelFile);
// initialize tensorflow inception model from modelFile
const net = cv.readNetFromTensorflow(modelFile);
const classifyImg = (img) => {
const white = new cv.Vec(255, 255, 255);
// ssdcoco model works with 300 x 300 images
const imgResized = img.resize(300, 300);
// network accepts blobs as input
const inputBlob = cv.blobFromImage(imgResized);
net.setInput(inputBlob);

// forward pass input through entire network, will return
// classification result as 1x1xNxM Mat
let outputBlob = net.forward();
// extract NxM Mat
outputBlob = outputBlob.flattenFloat(outputBlob.sizes[2],
    outputBlob.sizes[3]);

const results = Array(outputBlob.rows).fill(0)
    .map((res, i) => {
    const className = classNames[outputBlob.at(i, 1)];
    const confidence = outputBlob.at(i, 2);
    const topLeft = new cv.Point(
        outputBlob.at(i, 3) * img.cols,
        outputBlob.at(i, 6) * img.rows
    );
    const bottomRight = new cv.Point(
        outputBlob.at(i, 5) * img.cols,
        outputBlob.at(i, 4) * img.rows
    );
    return ({
        className, confidence,
        topLeft, bottomRight
    })
    });

    return results;
};
    </code></pre>
    </toc_code_segment>
    <page_break></page_break>
    <p>   The follwing figures (<toc_reference name="figure2c"></toc_reference> and <toc_reference name="figure2d">
        </toc_reference>)
        also from <a class="bibtex_reference" bibtexkey="muhler2017"></a> provide a good sense of the spacial (screen
        position) and linguistic (word class)
        outputs of the pretrained COCO SSD mdel:
    </p>
    <toc_figure id="figure2c" description="Spacial output of the COCO SSD object detection model.">
        <img src="./imgs/COCOSSD1.png" style=max-width:90%>
    </toc_figure>
    <toc_figure id="figure2d"
        description="Spacial and linguistic (word class) output of the COCO SSD object detection model.">
        <img src="./imgs/COCOSSD2.png" style=max-width:90%>
    </toc_figure>
    <page_break></page_break>
    <toc_section class=toc_section id="Audio Signal Analysis">
    </toc_section>
    <p>   This section, and the following section, explain the series of
        discoveries that caused us to switch from NPM/NodeJS to Python
        during our neural network based research. All of the code after
        the following section is written for Python 3.6.9.</p>
    <p>   For reference we have included <toc_reference name="code5"></toc_reference>
        which demonstrates how to load audio using python's ffmpeg library. We do not
        not use this method for the following <toc_reference name="Automatic Speech Recognition"></toc_reference>,
        but we do use it quite often later on in <toc_reference name="Audio Signal Time Series Prediction">
        </toc_reference>,
        so it is included here in the audio research section.
    </p>
    <toc_code_segment id="code5"
        description="Overview of the Python/ffmpeg audio file loading code, as well as how to use SciPy's 'rfft' function to get the real-valued frequency spectrum.">
        <pre><code class="python hljs">
import math
import numpy as np
import ffmpeg
from scipy.fftpack import rfft, fftshift
input_filename = 'test.mp3'
fft_size = 1024
sample_count = 0
buffer = np.array([])
dataframe, _ = (ffmpeg
    .input(input_filename)
    .output('-', format='s8', acodec='pcm_s8', ac=1, ar='44100')
    .overwrite_output()
    .run(capture_stdout=True)
)
length = math.floor(len(dataframe)/fft_size)
sample_count += length
dataframe = np.frombuffer(dataframe, np.byte)[0: fft_size*length]
buffer = np.concatenate((buffer, dataframe))

array = []
index = 0
def get_ffts(waveform):
    temp_chunks = np.split(waveform, sample_count)

    new_chunks = []
    for j in range(0, len(temp_chunks)):
        if len(temp_chunks[j]) >= fft_size:
            new_chunks.append(temp_chunks[j])
    channel_ffts = []
    for j in range(0, len(new_chunks)):
        chunk = new_chunks[j]
        freqs = rfft(chunk)
        freqs = freqs/fft_size
        channel_ffts.append(freqs)
    return channel_ffts

buffer = np.array(get_ffts(buffer))
        </code></pre>
    </toc_code_segment>
    <page_break></page_break>
    <toc_subsection class=toc_subsection id="Fast Fourier Transform">
    </toc_subsection>
    <p>   In addition to loading the raw time-domain signal amplitudes, <toc_reference name="code5"></toc_reference>
        also demonstrates how to obtain the real-valued frequency spectrum using the
        discrete fast fourier transform (DFFT) on the real valued audio signal input data
        sequences (time-domain). <a class="bibtex_reference" bibtexkey="scipy"></a> is the python module
        we used to do this, and the reference material has this to say about the `rfft` function used in <toc_reference
            name="code5"></toc_reference>:
    </p>
    <p style=" margin-left:10%;">
        When the DFT is computed for purely real input, the output is Hermitian-symmetric,
        i.e. the negative frequency terms are just the complex conjugates of the
        corresponding positive-frequency terms, and the negative-frequency
        terms are therefore redundant. This function does not compute
        the negative frequency terms and the length of the transformed axis
        of the output is therefore n/2 + 1.
    </p>
    <p>   The following figure <toc_reference name="figure3a"></toc_reference> from <a class="bibtex_reference"
            bibtexkey="wiki:xxx"></a>
        (which also gives a formal definition of the Fourier transform) exhibits the difference
        between the time domain and the frequency domain of a signal.
    </p>
    <toc_figure id="figure3a" description="Time (red) and frequency (blue) domains of an arbitrary signal.">
        <img src="./imgs/TimeFrequency.png" style=max-width:60%>
    </toc_figure>
    <p>   Later on in <toc_reference name="Audio Signal Time Series Prediction"></toc_reference> this
        technique of loading the frequency spectrum of audio files is used to turn a relatively small
        collection of music into a massive training data set. This data set is then split into training and
        validation data for a modified time series prediction model, in effect making a signal prediction
        tool (when used in combination with inverse DFFT to convert the frequency domain back into the time domain).</p>
    <page_break></page_break>
    <toc_subsection class=toc_subsection id="Automatic Speech Recognition">
    </toc_subsection>

    <p>   The following code segment from from <a class="bibtex_reference" bibtexkey="uberi_2019"></a>
        demonstrates the most effective process we found for
        running automatic speech recognition locally, and it used a tool called PocketSphinx, created by <a
            class="bibtex_reference" bibtexkey="bambocher_2018"></a>.</p>

    <toc_code_segment id="code6" description="Overview of the Python/PocketSphinx/ASR code.">
        <pre><code class="python hljs">
import argparse, torch, torchvision, wave, math, struct, sys
import speech_recognition as sr
AUDIO_FILE = "./video-audio.wav"
ap = argparse.ArgumentParser()
ap.add_argument("-v", "--video", required=True,
    help="path to the input video")
args = vars(ap.parse_args())
vframes, aframes, info = torchvision.io.read_video(args["video"],
     pts_unit='sec')
aframes = torch.div(torch.sum(aframes, dim=0), len(aframes))
aframes = torch.add(torch.mul(aframes, .5), .5)
aframes = torch.mul(aframes, 255)
aframes = aframes.char()
aframes = aframes.numpy()
fps = info["audio_fps"]
for i in range(0, math.floor(len(aframes)/fps)):
    wave_writer = wave.open(AUDIO_FILE, 'w')
    wave_writer.setnchannels(1) # mono
    wave_writer.setsampwidth(1)
    wave_writer.setframerate(info["audio_fps"])
    wave_writer.writeframesraw(bytes(aframes[i*fps:(i+1)*fps]))
    wave_writer.close()
    r = sr.Recognizer()
    h = None
    # recognize speech using Sphinx
    try:
    with sr.AudioFile(AUDIO_FILE) as source:
    audio = r.record(source)  # read the entire audio file
    h = r.recognize_sphinx(audio,show_all=True)
    for s in h.seg():
    print(s.word, s.prob
    except KeyboardInterrupt:
         exit()
    except:
        e = sys.exc_info()[0]
        print("Sphinx error; {0}".format(e))
        </code></pre>
    </toc_code_segment>
    <p>   Python bindings for PocketSphinx are provided as a part of a collection of ASR python
        tools in <a class="bibtex_reference" bibtexkey="uberi_2019"></a>, but it is the only
        tool in that collection which allows for offline transcribing. The other solutions
        readily available for ASR in python all required interaction with a
        online service for either training or inference.
    </p>
    <page_break></page_break>
    <toc_section class=toc_section id="Natural Language Processing">
    </toc_section>
    <p>   The absence of noise-proof ASR made it clear that we needed
        finer-grain control over the neural network creation and training processes.
        In order to do that, we decided to make a fuller commitment to using Pytorch and
        learning more about <a class="bibtex_reference" bibtexkey="tensorflow2015-whitepaper"></a> (a.k.a.
        TensorFlow/Keras),
        but first we had to learn more about the Python programming language iteself.
        Since we were also one modality short
        of making a truely multimodal model, so we decided to practice Python by
        exploring a new potential mode for our model: the linguistic mode.
    </p>
    <p>   Dense, real valued vectors representing distributional similarity information
        are now a cornerstone of practical natural language processing (NLP). Similarity
        is determined by comparing word vectors or “word embeddings”, multi-dimensional
        meaning representations of a word. We chose the Python spacy module to help
        us get up and running with word embeddings.
    </p>
    <toc_subsection class=toc_subsection id="Word Embeddings">
    </toc_subsection>
    <p>   <a class="bibtex_reference" bibtexkey="spaCy2020"></a> automatically downloads word-embedding models
        and stores them locally for future use. We chose a word vectors model that covers a
        huge vocabulary: en_vectors_web_lg. It provides 300-dimensional GloVe (Global
        Vectors for Word Representation) vectors for over 1 million terms of English. This
        was a good choice for modeling the complex relationship
        between a limited number of image classes and a high number of potential query strings.
    </p>

    <toc_code_segment id="code7" description="Overview of the Spacy/NLP code.">
        <pre><code class="python hljs">import spacy
nlp = spacy.load('en_vectors_web_lg')
tokens = nlp("dog cat banana")
for token1 in tokens:
    for token2 in tokens:
        print(token1.text, token2.text, token1.similarity(token2))
"""
Should output something like:
dog dog 1.0
dog cat 0.80168545
dog banana 0.24327643
cat dog 0.80168545
cat cat 1.0
cat banana 0.28154364
banana dog 0.24327643
banana cat 0.28154364
banana banana 1.0
"""
        </code></pre>
    </toc_code_segment>
    <page_break></page_break>
    <p>   The following figures (<toc_reference name="figure4a"></toc_reference> and <toc_reference name="figure5a">
        </toc_reference>) from <a class="bibtex_reference" bibtexkey="desagulier"></a>
        (which provides further definitions) are good visualizations of the concept behind word embeddings and their
        cosine similarities.</p>
    <toc_figure id="figure4a" description="Word vectors.">
        <img src="./imgs/Embeddings.jpg" style=max-width:80%>
    </toc_figure>
    <toc_figure id="figure5a" description="Cosine similarities of word vectors.">
        <img src="./imgs/Similarity.jpg" style=max-width:80%>
    </toc_figure>
</toc_chapter>
<toc_chapter class=toc_chapter id="Novel Multimodal Model Architecture for Event Detection">
    <p>   The following sections provide a detailed overview of the design and experiments of
        a novel multimodal prototype system for event detection in videos that was developed
        over the duration of this research.
    </p>
    <toc_section class=toc_section id="Architecture Design of the Multimodal System">
    </toc_section>

    <p>   The system in the following <toc_reference name="figure4"></toc_reference> is a representation the
        client-server application
        we created which combines the modalities of the frame images (from user-provided videos) with the their
        word embeddings (or multi-dimensional meaning representations of a word). This creates the
        desired ability for the end user to query complex concepts via the linguistic modality (rather
        than a series of individual image classifications) quickly and efficiently, with ever improving
        accuracy and precision with each refinement of a query.</p>
    <toc_figure id="figure4" description="Overview of the final framework.">
        <img src="./imgs/figure4.png" style=max-width:75%>
    </toc_figure>
    <page_break></page_break>

    <p>   Because the entirety of a user’s video content is preprocessed and stored in word
        vector
        embeddings upon uploading, binary search procedures of the entire domain of a user’s uploaded video content can
        be strategically reorganized categorically with rapid efficiency by
        the server system, and with very little effort by the end-user. This multimodal functionality
        ran with incredible efficiently on the prototype system.</p>
    <p>   The following <toc_reference name="table2a"></toc_reference> outlines the steps of preprocessing a video
        when one is uploaded to the server:
    </p>

    <toc_table style="text-align:center;" id="table2a"
        description="Steps for preprocessing videos uploaded to the server.">
        <table style="text-align:center; border-collapse:collapse;border-spacing:0" class="tg">
            <thead>
                <tr>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Step</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Description</th>
                </tr>
            </thead>
            <tbody style="text-align:center;">
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">1</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Receive the raw (usually mp4 encoded) video
                            files
                            on the server-side.
                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">2</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Save video to disk (under the uploader's
                            account folder).</span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">3
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Decode video frame-by-frame with ffmpeg.
                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">4
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Resize the video frames (299x299 for
                            InceptionV3).
                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">5
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Calculate the change of the RGB values
                            between each frame from step #4.
                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">6
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Calculate the standard deviation and
                            variation of the "delta frames" from step #5.
                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">7
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Find the middle of frame ranges that start
                            and end with frames whose RGB change are
                            withen atleast one standard deviation of the average change. This splits up the video into
                            periods of high-change (camera
                            panning, animations, actions) and low change (still and motionless scenes).
                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">8
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">For each successive range of frames
                            alternating depictions of
                            action and stillness (calculated in step #7) use the middle frame as input to Pytorch's
                            pretrained
                            InceptionV3 model.

                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">9
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">
                            Save the thumbnail, weights, label string, and label word vector/embedding for each of the
                            inferred
                            frames of step #8.
                        </span><br></td>
                </tr>
            </tbody>
        </table>
    </toc_table>
    <p>   In all of the experiments
        the preprocessing time of a video was usually about
        half the duration of the video itself. For example, a 10 minute long video usually
        takes about 5 minutes to preprocess.
    </p>
    <p>   The file size for the final result of preprocessing
        was approximately the same size as the uploaded video when using mp4 video format.

    </p>
    <p>   The following pseudo-<toc_reference name="code0"></toc_reference> outlines the steps for searching the
        preprocessed video files when a set of positive and negative query strings is sent to the server.
        The positive and negative input feature vectors are the sums of the positive and negative query
        strings as word vectors. We do not normalize the feature vectors because it seemed to lose information
        about occurance count (and cause queries to become inaccurate) when we tried.
    </p>

    <toc_code_segment id="code0" description="Pseudo-code for responding to a query sent to the server.">
        <pre><code class="python hljs"> 
        
def process_query(positive_feature_vector,\\
                negative_feature_vector,\\
                start_frame, end_frame, input_video):
    video_proto = Video(input_video)
    frames = video_proto.frames
    new_frames = []
    for frame_index in range(start_frame, end_frame):
        frame = frames[frame_index]
        start = frame.start
        end = frame.end
        word_vectors = frame.words
        word_scores = frame.visualScores
        new_frame = InfoFrame()
        similarity = 0.
        for i in range(0, len(word_vectors)):
            word = word_vectors[i].word
            vector = word_vectors[i].vector
            score = np.multiply(positive_feature_vector, vector)
            score = score.sum()
            probability = word_scores[i]
            if i &lt; query_result_depth:
                wv = WordVector()
                wv.word = word
                new_frame.words.append(wv)
                new_frame.visualScores.append(probability)
                new_frame.querySimilarityScores.append(score)
            similarity += score*probability
        dissimilarity = 0.
        for i in range(0, len(word_vectors)):
            word = word_vectors[i].word
            vector = word_vectors[i].vector
            score = np.multiply(negative_feature_vector, vector)
            score = score.sum()
            probability = word_scores[i]
            if i &lt; query_result_depth:
                new_frame.queryDisimilarityScores.append(score)
            dissimilarity += score*probability
        new_frame.start = start
        new_frame.end = end
        new_frame.positiveScore = similarity
        new_frame.negativeScore = dissimilarity
        new_frames.append(new_frame.SerializeToString())
    return new_frames 
    </code></pre>
    </toc_code_segment>
    <page_break></page_break>

    <toc_section class=toc_section id="How-To Usage Guide of the Multimodal System">
    </toc_section>
    <p>   The sole purpose of the GitHub repository/branch mentioned in the introduction
        (https://github.com/CrazedCoding/CrazedCoding.com/tree/thesis)
        is to serve as a platform for the thesis work of the author. So far, it is the culmination of a combination
        of many academic and entrepreneurial projects developed over the past several years.
    </p>
    <p>   The main component is a Linux-based python server that is capable of processing video
        files for keywords, visually. It also has the ability to efficiently (de)serialize
        messages sent to/from the client/server using Google Protobufs. It has a email based
        sign-up system, and uses TCP/WebSockets to send/receive messages to/from the client/server.
    </p>
    <p>   It is written for Python 3.7+ and it's requirements are listed in the 'requirements.txt'
        of the aforementioned GitHub repo/branch.
        The client is written in pure HTML/JavasSript and can be found in the www folder.
        To use this project for their own domain/server, the end user simply needs to replace
        all occurances of the domain name string "CrazedCoding" (while preserving the
        occurance's case) to their own server's domain name.
    </p>
    <p>   Server installation and configuration are documented in extensive detail on the GitHub
        repository and branch mentioned before. Once finished completing the step-by-step installation
        instructions, the end user should be able to start the
        server with the simple command: 'sudo python3 server.py'.
    </p>
    <p>   Depending on what the end user's network setup is like, they should be able to open a browser
        to view the client-side of their newly configured video-processing system. The first thing that should
        done completing these steps is to create an account and log in using
        the forms of the client-side system, shown in <toc_reference name="figure6"></toc_reference> (left).
    </p>
    <p>   For the purpose of this How-To, we have already signed up using a temporary
        email address with the user name "bituser", which will appear in the following
        example figures.
    </p>
    <toc_figure id="figure6" description="Screenshots of the signup (left) and login (right) pages of the site.">
        <img src="./imgs/figure7.png" style="max-width:49%; display: inline;">
        <img src="./imgs/figure8.png" style="max-width:49%; display: inline;">
    </toc_figure>
    <p>   This part of the site was the result of our obligation to isolate
        potentially sensitive user video content to just the person who uploaded it.
        We've effectively taken every measure possible to make video sharing between different users
        of the site impossible, unless they share a username and email or password.
    </p>
    <p>   Once someone creates an account using a valid email address, they can log in,
        and when they do they can access new options. These options are shown
        in <toc_reference name="figure7"></toc_reference>.
    </p>
    <toc_figure id="figure7"
        description="Options shown on the home page before (left) and after (right) logging in with an account associated with a verified email address.">
        <img src="./imgs/figure6.png" style="max-width:49%; display: inline;">
        <img src="./imgs/figure9.png" style="max-width:49%; display: inline;">
    </toc_figure>
    <p>   As can be seen in <toc_reference name="figure7"></toc_reference> above, the options
        for someone who is verified and logged in are slightly different from someone who is not. The main
        differences are the "Upload", "Query", "Profile", and "Logout" buttons.</p>
    <p>   When the user presses the "Uploads" button they are taken to the uploads section of
        their account. The "Uploads" section includes a "Choose Video" button which the user can
        use to choose a video file on their local file system, and then upload it to the server
        for preprocessing. The "Uploads" page of our example account
        is shown in <toc_reference name="figure8"></toc_reference>.
    </p>
    <p>   Our example account has uploaded four (4) videos of varying content and duration.
        The four test videos account for 17 minutes of video. Every minute of video took
        about half a minute minute of processing time during preprocessing.
    </p>
    <toc_figure id="figure8" description="Options shown on the uploads page for the example account.">
        <img src="./imgs/figure10.png" style="max-width:70%; display: inline;">
    </toc_figure>
    <p>   In <toc_reference name="figure8"></toc_reference> above, the test user's
        uploaded video list is visisble, including:
    </p>
    <ul>
        <li><b>2.mp4 </b>(9min. 39sec.) - video of a news segment about
            California wildfires; </li>
        <li><b>1.mp4 </b>(1min. 49sec.) - aerial video of three different homes
            burning during a forest fire; </li>
        <li><b>cats_and_dogs.mp4 </b>(3min. 8sec) - video of cats, dogs,
            and sometimes other animals; </li>
        <li><b>Vietnam War - Music Video - Break on Through.mp4 </b>
            (2min. 28sec.) - music video by The Doors featuring Vietnam war footage.
        </li>
    </ul>
    <page_break></page_break>
    <p>   The test user's "Query" page is shown in <toc_reference name="figure9"></toc_reference>
        below (left). Once a user has uploaded content, they can create frame-by-frame
        queries by clicking the "Query" button in the menu at the top of the page.
        The page shown after clicking "Create Visual Query" on this page is shwon in
        <toc_reference name="figure9"></toc_reference> below (right).
    </p>
    <toc_figure id="figure9"
        description="Options shown on the query (left) and create query (right) pages for the example account.">
        <img src="./imgs/figure11.png" style="max-width:49%; display: inline;">
        <img src="./imgs/figure12.png" style="max-width:49%; display: inline;">
    </toc_figure>
    <p>   The "Create Visual Query" dialogue allows the user to specify <b>positive</b>
        and <b>negative</b> query keyworkds. The positive and negative query keywords
        are used to sort the visual content of the "Uploads" videos list, frame-by-frame.
        The exact mechanisms for this entire process are described
        <toc_reference name="Architecture Design of the Multimodal System"></toc_reference>
    </p>
    <page_break></page_break>
    <toc_figure id="figure10"
        description="Options shown on the create query page (left), and resulting loading pages (right). Querying 17 minutes of video takes about 2 seconds.">
        <img src="./imgs/figure13.png" style="max-width:49%; display: inline;">
        <img src="./imgs/figure14.png" style="max-width:49%; display: inline;">
    </toc_figure>
    <p>   On the following page, in <toc_reference name="figure11"></toc_reference>,
        the results to the above query are returned. The videos are ordered starting from
        the one with the highest best scoring frame to the one with the lowest best scoring
        frame. The scores of the frames of the videos are the same as those calculated
        in <toc_reference name="code0"></toc_reference>.
    </p>
    <p>   Notice that the thumbnails for each of the videos has changed to be
        the frame of highest-scoring frame of each video according to the query; in
        particular the top result shows a thumbnail of a german shepard breed of dog.
        We would like to note that this type of behavior was consistent for highly
        specific nouns, like "fire", "truck", "walrus", but somewhat inaccurate for
        less descriptive words like "danger".
    </p>

    <page_break></page_break>
    <toc_figure id="figure11" description="Results returned from the query mentioned previously.">
        <img src="./imgs/figure15.png" style="max-width:90%; display: inline;">
    </toc_figure>
    <page_break></page_break>
    <br>
    <br>
    <p>   In <toc_reference name="table2"></toc_reference> below
        are the video-by-video results to the previous query. The videos
        are sorted according to their best-scoring frame's score. Notice
        that even though the news about fires (row 3) has a better overall
        score than the war footage does, it is still ranked lower because
        it's best ranked frame was still less.
    </p>
    <toc_table style="text-align:center;" id="table2"
        description="Search query results for positive query words 'dog' and 'puppy' as well as negative query words 'cat' and 'kitten'. Relavence of the best frame and entire video are under the score sections.">
        <table style="text-align:center; border-collapse:collapse;border-spacing:0" class="tg">
            <thead>
                <tr>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Video Name</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Video Content</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Best Score</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Overall Score</th>
                </tr>
            </thead>
            <tbody style="text-align:center;">
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">
                            cats_and_dogs.mp4
                        </span><br>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">
                            cats, dogs, sea-lions, penguins
                        </span>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">1620.53</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">267.76</span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Vietnam War - Music Video - Break on
                            Through.mp4</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">war footage</span><br>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">1341.74</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">136.98</span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">2.mp4</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">news segment about fires</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">599.71
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">182.97
                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">1.mp4</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">three seperate house fires</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">96.8
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">23.07
                        </span><br></td>
                </tr>
            </tbody>
        </table>
    </toc_table>
    <p>   The "Score-Ordered Preview" buttons visible in the query result
        entries of <toc_reference name="figure11"></toc_reference> open
        the dialogue shown in <toc_reference name="figure12"></toc_reference> on
        the next page.
    </p>
    <p>   At the top of the "Score-Ordered Preview" dialogue
        is a table listing the label strings (from the
        InceptionV3 model), label visibility scores,
        the label similarty scores (based on the <b>positive</b> query
        strings), and the label difference scores (based on the
        <b>negative</b> query strings).
    </p>
    <p>   In the middle of the "Score-Ordered Preview" dialogue
        is a preview of the video. The user can click this to
        start and pause the video, as well as navigate randomly
        throughout the video.
    </p>
    <p>   The last item of the "Score-Ordered Preview" dialogue
        is a preview of the frame scores. The height above the top and lower
        parts of the graph indicate the frame scores.
        The order of the top scores is the original video frame sequence.
        The order of the bottom scores is based on each frame range's
        frame score: if a continuous range of frame scores is higher than the average
        score then the entire range is not seperated.
    </p>
    <page_break></page_break>
    <p>   The blue/purple/pink/red
        colors of the lines at the bottom are actually a linear interpolation from blue-to-red
        based on the frame-by-frame scores. The highest scoring frame is blue and
        the lowest scoring frame is red.
    </p>
    <toc_figure id="figure12" description="The first frame being previewed by the 'Score-Ordered Preview'.">
        <img src="./imgs/figure16.png" style="max-width:95%; display: inline;">
    </toc_figure>
    <page_break></page_break>
    <p>   If we move the time navigation bar from the pink area to
        the blue area (by clicking the blue area)
        we start to see video of the german shepard we saw in the
        thumbnail. It is the frame with the highest ranking score.
    </p>
    <toc_figure id="figure13"
        description="The highest ranked frame of this video in relation to the query. The InceptionV3 words and their cosine similarities to the query are at the top. The time range of the frames containing the dog is marked in blue in bottom.">
        <img src="./imgs/figure17.png" style="max-width:90%; display: inline;">
    </toc_figure>
    <page_break></page_break>
    <p>   If we move the time navigation bar from the blue area to
        the red area (by clicking the red area)
        we start to see video of a cat.
        It is the frame with the lowest ranking score in a query
        results to a query where cats are ranked negatively.
    </p>
    <toc_figure id="figure14" description="A cat in the lowest ranked frame of the video.">
        <img src="./imgs/figure18.png" style="max-width:95%; display: inline;">
    </toc_figure>
    <page_break></page_break>
    <p>   If we click the "End Scoring Preview", "Clear Query Results", and
        then "Create Visual Query", then we can reconfigure our last query.
        It has been shown that the query system can differentiate betwee cats and dogs, so
        we will now demonstrate how it can identify threats. In <toc_reference name="figure15"></toc_reference>
        the query form has been reconfigured to test this.
    </p>
    <toc_figure id="figure15" description="A refined query for finding threats.">
        <img src="./imgs/figure19.png" style="max-width:95%; display: inline;">
    </toc_figure>
    <p>   On the following page, in <toc_reference name="figure16"></toc_reference>,
        the results to the above query are returned. Notice that the thumbnails
        for each of the videos has changed to be the highest-scoring
        frame of each video according to the query; in particular the top result
        shows a thumbnail of jets.
    </p>
    <page_break></page_break>
    <toc_figure id="figure16"
        description="Search query results for positive query words 'bomb', 'tank', 'jet' and 'helicopter', as well as negative query words 'cat', 'kitten', 'dog' and 'puppy'. Relavence of the best frame and entire video are under the score sections.">
        <img src="./imgs/figure20.png" style="max-width:95%; display: inline;">
    </toc_figure>
    <page_break></page_break>
    <br>
    <br>
    <p>   In <toc_reference name="table2b"></toc_reference> below
        are the results to the previous query. In general, the scores all have much
        higher absolute values because the number of query vectors
        has increased.
    </p>
    <toc_table style="text-align:center;" id="table2b"
        description="Video-by-video query results for positive query words 'bomb', 'tank', 'jet' and 'helicopter', as well as negative query words 'cat', 'kitten', 'dog' and 'puppy'. Relavence of the best frame and entire video are under the score sections.">
        <table style="text-align:center; border-collapse:collapse;border-spacing:0" class="tg">
            <thead>
                <tr>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Video Name</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Video Content</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Best Score</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Overall Score</th>
                </tr>
            </thead>
            <tbody style="text-align:center;">
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Vietnam War - Music Video - Break on
                            Through.mp4</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">war footage</span><br>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">6289.36</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">1288.85</span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">1.mp4</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">three seperate house fires</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">2739.72
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">1354.61
                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">2.mp4</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">news segment about fires</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">2676.55
                        </span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">229.01
                        </span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">
                            cats_and_dogs.mp4
                        </span><br>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">
                            cats, dogs, sea-lions, penguins
                        </span>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">1738.18</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">-1907.43</span><br></td>
                </tr>
            </tbody>
        </table>
    </toc_table>
    <p>   The overall score column reveals that the query system is capable
        of correctly identifying threats from a set of query words. Unfourtunately,
        those query words had to be specific enough to match an object in the
        scene containing a threat (most of the time).
    </p>
    <p>   Interestingly, though, the query system seemed to be able to discern
        that videos of fire had threat scores somewhere in between videos
        of small animals and videos of war marchines, even though our query did not
        contain the exact word "fire".
    </p>
    <p>   It is also interesting that the news videos of fire had a much lower overall
        threath score than the raw aerial footage of homes burning.
    </p>
    <p>   The results of these experiments have shown that the query system
        has at least some semblance of accuracy and precision when querying
        videos frame-by-frame for visual content based on query strings.
    </p>
    <p>   Since reading about a video application isn't entirely practical,
        <a class="bibtex_reference" bibtexkey="youtube2020"></a> has been included in
        the references section, and it is a YouTube video
        of this experiment.
    </p>
</toc_chapter>
<toc_chapter class=toc_chapter id="Audio Signal Time Series Prediction">
    <p>   The following sections provide a detailed overview of the design and experiments of
        an audio signal time series prediction prototype system that was developed
        over the duration of this research.
    </p>
    <toc_section class=toc_section id="Architecture Design of the Predction System">
    </toc_section>
    <p>   The system in the following <toc_reference name="figure5"></toc_reference> is a representation the Python
        application
        we created for learning and predicting audio signal frequencies. As input it takes:
        (1) a list of input sound files to learn, (2) an optional weights file from previous
        batches (a new weights file is generated every time the system trains), (3) a list
        of target sound files to try to predict, and (4) an output file to store the resulting
        predictions. The frequency histogram loading code was already briefly covered in
        <toc_reference name="code5"></toc_reference>.
    </p>
    <toc_figure id="figure5" description="Overview of the audio signal time series prediction system.">
        <img src=./imgs/figure5.png style=max-width:100%>
    </toc_figure>
    <page_break></page_break>
    <p>   You can find a copy of this program in the form of 'audio_processing.py',
        executable command-line tool and Python source code found in the 'audio-frequency-prediction-tool'
        folder of this thesis' accompanying GitHub repo/branch.
    </p>
    <p>   The 'audio_processing.py' command-line tool depends on the 'audio_data.py' 
        file for the audio signal data loading, as well as the 'audio_model.py' file
        for the Pytorch model loading/training/inference. The 
        <toc_reference name="figure8"></toc_reference> below shows the layers of our
        unimodal time series prediction model. It takes 15 steps of 1024-sized FFT's of 
        raw signle-channel audio signal time-domain samples of the input songs and
        eventually tries to predict the next step in the sequence.
    </p>
    <toc_figure id="figure8" description="Overview of the prediction model.">
        <img src="./imgs/AudioModel.png" style=max-width:50%>
    </toc_figure>
    <p>   The model has 6 layers. The layers, and parameters to each, can be
        configured by modifying 'config.json' in the 'audio-frequency-prediction-tool'
        folder.
    </p>
    <p>   In the following section we will discuss how to use the 'audio_processing.py'
        command-line tool in detail, and show the results to an experiment
        that demonstrates the effectiveness of the tool.
    </p>
    <page_break></page_break>
    <toc_subsection class=toc_section id="How-To Usage Guide of the Predction System">
    </toc_subsection>
    <p>   Below, in <toc_reference name="code7"></toc_reference>, is the output of a terminal
        session that starts with the execution of 'audio_processing.py'. It gives the program
        three (3) parameters: (1) the input song, by <a class="bibtex_reference" bibtexkey="youtube2012"></a>,
        to turn into a frequency histogram and learn,
        (2) the song to listen to and try and predict (point-by-point), and (3) the name of 
        the file to write the output to.
    </p>
    <p>   The program takes about 15 minutes to run, and generated a file named "28052020-152009-e4.h5"
        which is the weights file of our model and can be reused by giving the command-line tool
        the "-m" (or "--model") parameter to specify the weights file. using it's recently updated
        model, the program runs point-by-point inference against the same input sound file and
        saves the predictions to the output file ("output.mp3").
    </p>
    <toc_code_segment id="code7" description="Terminal output for a trial run of the audio prediction program.">
        <pre><code class="python hljs" style="font-size: 9pt;">
> sudo python3 audio_processing.py -i "Turret Opera Capella Version.mp3"\
-t "Turret Opera Capella Version.mp3" -o output.mp3

Loading config.json settings file.
Building NN model.
[Model] Model Compiled
Loading audio training input file(s).
Loading audio training input file: ['Turret Opera Capella Version.mp3']

.... A lot of ffmpeg-python output ....

Reformatting audio data.
Training model with audio data.
[Model] Training Started
[Model] 4 epochs, 32 batch size
Epoch 1/4
4319/4319 [==============================] - 84s 19ms/step - loss: 0.0268
Epoch 2/4
4319/4319 [==============================] - 77s 18ms/step - loss: 0.0261
Epoch 3/4
4319/4319 [==============================] - 77s 18ms/step - loss: 0.0251
Epoch 4/4
4319/4319 [==============================] - 78s 18ms/step - loss: 0.0238
[Model] Training Completed. Model saved as ./28052020-152009-e4.h5
Loading target audio files for point-by-point prediction.
Loading target audio file: Turret Opera Capella Version.mp3.

.... A lot of ffmpeg-python output ....

Merging target audio files with randomly sinusoidally changing weights.
Using current NN model to do point-by-point prediction of the merged target
audio files.
[Model] Predicting Point-by-Point...
4319/4319 [==============================] - 22s 5ms/step
Constructing waveform out of final point-by-point prediction.
Saving/encoding audio file to file: output.mp3

>
</code></pre>
    </toc_code_segment>

    <page_break></page_break>
    <p>   The results of running this program are nothing short of fascinating (and
        fun to play with). We have taken the liberty of leaving the input and
        output sound files of the experiment in the 'audio-frequency-prediction-tool'
        folder of the master branch of the accompanying GitHub repo. We did not
        leave '28052020-152009-e4.h5' because it was 150MB.
    </p>
    <p>   For readers unable to actually listten to the input and output sound files
        included in the GitHub repo 'thesis' branch, we have included the following
        waveform plots of the time-domain of input and output signals below in 
        <toc_reference name="figure9"></toc_reference>.
    </p>
    <toc_figure id="figure9" description="The input (blue) and output (blue) time-domains of the input and output sound files of the audio prediction tool experiment.">
        <img src="./imgs/TimeIn.png" style="position: relative; width:100%; height:100%">
        <img src="./imgs/TimeOut.png" style="position: relative; width:100%; height:100%">
    </toc_figure>
    <p>   The audio prediction model is very good at predicting the original signal, and sometimes
        even creating interesting and new sounds by having it try to predict sounds it has never
        heard before.
    </p>
    <p>   Developing this program helped beat the learning curve for
        implementing sequental/LSTM/prediction Python Keras/Tensorflow models by
        periodically rewarding the developer with a sound that either validated or
        invalidated their progress.
    </p>
</toc_chapter>
<toc_chapter class=toc_chapter id="Conclusion and Future Work">
    <p>   The answers to our questions and hypthoses are listed in <toc_reference name="table1">
    </toc_reference>, and most of our hyptheses about neural networks and their
    potential were correct, with notable exception of ASR, which is currently mostly comprised
    of server-side solutions.
    The topic of ASR will certainly be revisitted after researching denoising techniques, which have the
    potential of increasing the accuracy of the handful of client-side ASR solutions.</p>
    <p>   The loss of ASR as a modality to the originally designed framework ultimately pushed
        us in the direction of switching from NPM/NodeJS to Python.</p>
    <p>   The final multimodal Python
        video processing server used visual and linguistic modes of input to obtain results of
        queries generated by the user. This was all done to a satisfactory degree of precision
        and accuracy, as the final interface proved to be easily managable, fast, efficient,
        and produced generally correct results.
    </p>
    <p>   The final audio signal prediction tool was just the innovative type of project we 
        needed in order to replace the lack of audio research being done as a result of losing
        ASR as a solution. Developing this tool helped beat the learning curves of Python, Pytorch, 
        and Keras/Tensorflow, thereby enabling us to do more in-depth and comprehensive neural 
        network experiments in the future, and creating an example for others to learn from.
    </p>
    <p>   Much of this research was done on an extremely strict and short schedule, and in the 
        future more time will be spent developing the associated GitHub project. It is highly 
        recommended that interested readers follow the 'master' branch of the final project
        for updates in the future.
    </p>
</toc_chapter>
<toc_chapter class=toc_chapter id="References">
    <div class="bibtex_display" id="references_section" callback="window.bibtex_callback(bibtex_display)"
        style="text-align: left;"></div>
</toc_chapter>
<script>
    var subpage_height_mm = 237;
    var real_page_index = 1
    var pxTomm = function (px) {
        return Math.floor(px / ($('#100_mm_ruler').height() / 100)); //JQuery returns sizes in PX
    };
    var subpage_header = `
                <div style="position: relative; top:-1cm; margin-bottom:-1cm; text-align: center;"><span style="font-style:normal;font-weight:normal;font-size:10pt;font-family:Times New Roman;color:#000000">Beijing Institute of Technology Master Degree Thesis</span><span style="font-style:normal;font-weight:normal;font-size:10pt;font-family:Times New Roman;color:#000000"> </span><br/></SPAN></div>
                <img style="position: relative; top:-1.35cm; margin-bottom:-1cm; text-align: center;" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAk8AAAACCAYAAABBnjj1AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAAfSURBVFhH7cOxDQAADAIg/3/anmC6Q0IAAPirqqprek3dS9EBwTMwAAAAAElFTkSuQmCC" />
                <br>
                `

    function romanize(num) {
        if (isNaN(num)) return NaN;
        var digits = String(+num).split(""),
            key = ["", "C", "CC", "CCC", "CD", "D", "DC", "DCC", "DCCC", "CM", "", "X", "XX", "XXX", "XL", "L", "LX", "LXX", "LXXX", "XC", "", "I", "II", "III", "IV", "V", "VI", "VII", "VIII", "IX"],
            roman = "",
            i = 3;
        while (i--) roman = (key[+digits.pop() + (i * 10)] || "") + roman;
        return Array(+digits.join("") + 1).join("M") + roman;
    }

    window.onload = function () {
        var toc_fm_pages = []
        var toc_fm_header_strings = []
        var toc_fm_header_elements = []
        $(".toc_frontmatter_entry").each(function (index) {
            var main_page = $(this);
            toc_fm_header_strings.push(main_page.attr('id'))
            var children = []
            main_page.children().each(function (index) {
                children.push($(this));
                main_page.remove($(this))
            });
            var last_main_page = $(document.createElement("div"))
            last_main_page.addClass("page")
            $(this).append(last_main_page)
            var last_sub_page = $(document.createElement("div"))
            last_sub_page.addClass("subpage")
            last_sub_page.append(subpage_header);
            var last_sub_page_content = $(document.createElement("div"))
            last_sub_page_content.addClass("subpage_content")

            var toc_header = $('<h2 style="text-align:center; font-size: 20pt"></h2>')
            toc_fm_header_elements.push(toc_header)
            last_sub_page.append(toc_header)
            last_sub_page.append(last_sub_page_content)
            last_main_page.append(last_sub_page);
            toc_fm_pages.push(last_sub_page)
            window.total_height = last_sub_page.outerHeight()
            while (children.length > 0) {
                element = children.shift()
                last_sub_page_content.append(element)
                window.total_height += element.outerHeight();
                console.log(window.total_height)
                if (pxTomm(last_sub_page_content.height()) > subpage_height_mm) {
                    last_sub_page_content.remove(element)
                    children.unshift(element)
                    last_main_page = $(document.createElement("div"))
                    last_main_page.addClass("page")
                    $(this).append(last_main_page)
                    last_sub_page = $(document.createElement("div"))
                    last_sub_page.addClass("subpage")
                    last_sub_page.append(subpage_header)
                    var last_sub_page_content = $(document.createElement("div"))
                    last_sub_page_content.addClass("subpage_content")
                    last_sub_page.append(last_sub_page_content)

                    last_main_page.append(last_sub_page);
                    toc_fm_pages.push(last_sub_page)
                    window.total_height = last_sub_page.outerHeight()
                }
            }
        });
        var toc_chapters = []
        var toc_chapter_pages = []
        var toc_chapter_names = []
        var toc_chapter_elements = []
        var page_number = 1;
        $(".toc_chapter").each(function (index) {
            var main_page = $(this);
            toc_chapter_names.push(main_page.attr('id'))
            var accumulation = []
            var children = []
            main_page.children().each(function (index) {
                children.push($(this));
                main_page.remove($(this))
            });
            var last_main_page = $(document.createElement("div"))
            last_main_page.addClass("page")
            last_main_page.attr('number', page_number++);
            $(this).append(last_main_page)
            var last_sub_page = $(document.createElement("div"))
            last_sub_page.addClass("subpage")
            last_sub_page.append(subpage_header);
            var last_sub_page_content = $(document.createElement("div"))
            last_sub_page_content.addClass("subpage_content")

            var toc_header = $('<h2 style="text-align:center; font-size: 20pt"></h2>')
            toc_chapter_elements.push(toc_header)
            last_sub_page.append(toc_header)
            last_sub_page.append(last_sub_page_content)
            last_main_page.append(last_sub_page);
            toc_chapters.push(last_sub_page)
            toc_chapter_pages.push(last_sub_page)
            window.total_height = last_sub_page.outerHeight()
            while (children.length > 0) {
                element = children.shift()
                last_sub_page_content.append(element)
                window.total_height += element.outerHeight();
                console.log(window.total_height)
                if (element.is("page_break") || pxTomm(last_sub_page_content.height()) > subpage_height_mm) {
                    last_sub_page_content.remove(element)
                    if (!element.is("page_break"))
                        children.unshift(element)
                    last_main_page = $(document.createElement("div"))
                    last_main_page.addClass("page")
                    last_main_page.attr('number', page_number++);
                    $(this).append(last_main_page)
                    last_sub_page = $(document.createElement("div"))
                    last_sub_page.addClass("subpage")
                    last_sub_page.append(subpage_header)
                    var last_sub_page_content = $(document.createElement("div"))
                    last_sub_page_content.addClass("subpage_content")
                    last_sub_page.append(last_sub_page_content)

                    last_main_page.append(last_sub_page);
                    toc_chapter_pages.push(last_sub_page)
                    window.total_height = last_sub_page.outerHeight()
                }
            }
        });
        var toc_names = []
        var toc_depths = []
        var depth = 0;
        for (var i = 0; i < toc_fm_header_elements.length; i++) {
            toc_fm_header_elements[i].text(toc_fm_header_strings[i])
        }
        for (var i = 0; i < toc_chapter_elements.length; i++) {
            toc_chapter_elements[i].text((toc_chapter_names[i].toLowerCase() != "references" && toc_chapter_names[i].toLowerCase() != "conclusion and future work" ? 'Chapter ' + (i + 1) + " - " : '') + toc_chapter_names[i])
        }

        var toc_list = $("#toc_list");
        var dots = '........................................................................................................................';

        for (var i = 0; i < toc_fm_pages.length; i++) {
            toc_list.append($('<div><div style="float:left; display:inline-block; overflow: hidden; white-space: nowrap;max-width:92.5%;text-overflow:ellipsis;"><p><a href="#' + toc_fm_header_strings[i] + '">' + toc_fm_header_strings[i] + "</a>" + dots + '</p></div><div style="min-width:5%;text-align:center;display:inline-block;"><p  style="text-align:center;">' + romanize(i + 1) + '</p></div></div>'));
        }

        var chapter = 0;
        var section = 0;
        var subsection = 0;
        var figure = 0;
        var table = 0;
        var code_segment = 0;

        var is_chapter = false;
        var is_section = false;
        var is_subsection = false;

        $(".toc_chapter, .toc_section, .toc_subsection, toc_figure, toc_table, toc_code_segment").each(function (index) {
            if ($(this).hasClass("toc_chapter")) {

                is_chapter = true;
                is_section = false;
                is_subsection = false;

                var page = 0
                var parent = $(this)
                var children = parent.children()
                if (children.hasClass("page")) {
                    page = children.attr('number');
                }

                var name = $(this).attr('id');
                chapter += 1;
                section = 0;
                subsection = 0;
                figure = 0;
                table = 0;
                code_segment = 0;
                toc_list.append($(
                    '<div> <div style="float:left; display:inline-block; overflow: hidden; white-space: nowrap;max-width:92.5%;text-overflow:ellipsis;">' +
                    '<p><a href="#' + name + '">' + (name.toLowerCase() != "references" && name.toLowerCase() != "conclusion and future work" ? 'Chapter ' + chapter + '.&ensp;' : '') + name + "</a>" + dots + '</p></div>' +
                    '<div style="min-width:5%;text-align:center;display:inline-block;"><p style="text-align:center;">' + page + "</p></div></div>"));
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>chapter " + chapter + "</a>");
                });
            }
            if ($(this).hasClass("toc_section")) {
                is_chapter = false;
                is_section = true;
                is_subsection = false;
                var page = 0
                var parent = $(this)
                while (parent = parent.parent()) {
                    if (parent.hasClass("page")) {
                        page = parent.attr('number');
                        break;
                    }
                }

                var name = $(this).attr('id');
                section += 1;
                subsection = 0;
                toc_list.append($(
                    '<div><div style="float:left; display:inline-block; overflow: hidden; white-space: nowrap;max-width:92.5%;text-overflow:ellipsis;"><p>' +
                    '&emsp;&emsp;<a href="#' + name + '">Section ' + chapter + "." + section + '&ensp;' + name + '</a>' + dots + '</p></div><div style="min-width:5%;text-align:center;display:inline-block;"><p style="text-align:center;">' +
                    page + "</p></div></div>"));

                $(this).html("<h3>" + chapter + "." + section + " - " + name + "</h3>");
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>section " + chapter + "." + section + "</a>");
                });
            }
            if ($(this).hasClass("toc_subsection")) {
                is_chapter = false;
                is_section = false;
                is_subsection = true;
                var page = 0
                var parent = $(this)
                while (parent = parent.parent()) {
                    if (parent.hasClass("page")) {
                        page = parent.attr('number');
                        break;
                    }
                }
                var name = $(this).attr('id');

                subsection += 1;
                toc_list.append($(
                    '<div><div style="float:left; display:inline-block; overflow: hidden; white-space: nowrap;max-width:92.5%;text-overflow:ellipsis;"><p>' +
                    '&emsp;&emsp;&emsp;<a href="#' + name + '">Subsection ' + + chapter + "." + section + "." + subsection + '&ensp;' + name + '</a>' + dots + '</p></div><div style="min-width:5%;text-align:center;display:inline-block;"><p style="text-align:center;">' +
                    page + "</p></div></div>"));

                $(this).html("<h3>" + chapter + "." + section + '.' + subsection + " - " + $(this).attr('id') + "</h3>");
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>subsection " + chapter + "." + section + '.' + subsection + "</a>");
                });
            }
            if ($(this).is("toc_figure")) {

                var name = $(this).attr('id');
                figure += 1;

                $(this).append($("<p style='text-align:center;'><b>Figure " + chapter + "." + figure + "</b>: " + $(this).attr('description') + "</p>"));
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>figure " + chapter + "." + figure + "</a>");
                });
            }
            if ($(this).is("toc_table")) {
                var name = $(this).attr('id');
                table += 1;
                $(this).prepend($("<p style='text-align:center;'><b>Table " + chapter + "." + table + "</b>: " + $(this).attr('description') + "</p>"));
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>table " + chapter + "." + table + "</a>");
                });
            }
            if ($(this).is("toc_code_segment")) {
                var name = $(this).attr('id');
                code_segment += 1;
                $(this).prepend($("<p style='text-align:center;'><b>Code Segment " + chapter + "." + code_segment + "</b>: " + $(this).attr('description') + "</p>"));
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>code segment " + chapter + "." + code_segment + "</a>");
                });
            }
        });

        $(".toc_chapter").children().each(function (index) {
            if ($(this).hasClass("page")) {
                $(this).children().append($("<div style='position: absolute; left:48%; bottom:-2cm;'><p style='text-align:center;'>" + (real_page_index++) + "</p></div>"));
            }
        });

        toc_page = toc_list.parent().parent();

        var main_page = toc_page.children().children();
        var children = []
        main_page.children().each(function (index) {
            children.push($(this));
            main_page.remove($(this))
        });
        children.unshift($('<h2 style="text-align:center; font-size: 20pt">Table of Contents</h2>'))
        main_page = main_page.parent().parent().parent().parent();
        main_page.children().remove()
        var last_main_page = $(document.createElement("div"))
        last_main_page.addClass("page")
        $(".table_of_contents").remove($(".table_of_contents").children())
        main_page.append(last_main_page)
        var last_sub_page = $(document.createElement("div"))
        last_sub_page.addClass("subpage")
        last_sub_page.append(subpage_header);
        var last_sub_page_content = $(document.createElement("div"))
        last_sub_page_content.addClass("subpage_content")
        last_sub_page.append(last_sub_page_content);
        last_main_page.append(last_sub_page);
        window.total_height = last_sub_page.outerHeight()
        while (children.length > 0) {
            element = children.shift()
            last_sub_page_content.append(element)
            window.total_height += element.outerHeight();
            console.log(window.total_height)
            if (pxTomm(last_sub_page_content.height()) > subpage_height_mm) {
                last_sub_page_content.remove(element)
                children.unshift(element)
                last_main_page = $(document.createElement("div"))
                last_main_page.addClass("page")
                main_page.append(last_main_page)
                last_sub_page = $(document.createElement("div"))
                last_sub_page.addClass("subpage")
                last_sub_page.append(subpage_header)
                var last_sub_page_content = $(document.createElement("div"))
                last_sub_page_content.addClass("subpage_content")
                last_sub_page.append(last_sub_page_content)
                last_main_page.append(last_sub_page);
                window.total_height = last_sub_page.outerHeight()
            }
        }
        window.bibtex_callback = update_references
        createWebPage(current_template, true)
    }
    var references_bibtex_display = null;
    var references_bibtex_display_parent = null;
    function update_references(bibtex_display) {
        $("toc_frontmatter_entry").children().each(function (index) {
            $(this).children().append($("<div style='position: absolute; left:48%; bottom:-2cm;'><p style='text-align:center;'>" + romanize(index + 1) + "</p></div>"))
        });
        $('.bibtexentry').removeClass("bibtexentry")
        bibtex_display = $(bibtex_display)
        references_bibtex_display_parent = bibtex_display.parent()
        bibtex_display.remove()
        $('.bibtex_reference').addClass("bibtex_display")
        references = true;
        $('.bibtex_template').remove()
        references_bibtex_display = bibtex_display;
        window.bibtex_callback = update_references_section
        references_bibtex_display_parent.append($('<div id="phantom_bibtex_display" class="bibtex_display" callback="window.bibtex_callback(bibtex_display)"style="text-align: left;"></div>'))
        createWebPage(`
        <a class=\"bibtex_template\">[<span class="entry_index"></span>] <span class="author" max=1><span class="von"></span> <span class="last"></span><span class="junior"></span></span></a>`, false);
    }
    function update_references_section(bibtex_display) {

        // var bibstring = "";
        // var requests = [];
        // if ($("#bibtex_input").length) {
        //     bibstring += $("#bibtex_input").val();
        // }
        // var b = new BibtexParser();
        // b.setInput(bibstring);
        // $(".bibtexentry").each(()=>{
        //     if($(this).parent().attr('bibtexkey') 
        // })

        $("#phantom_bibtex_display").remove();
        references_bibtex_display_parent.append(references_bibtex_display)
        hljs.initHighlighting();

        ref_page = $("#references_section").parent();
        var main_page = ref_page.children();
        var children = []
        main_page.children().each(function (index) {
            children.push($(this));
            main_page.remove($(this))
        });
        children.unshift($('<h2 style="text-align:center; font-size: 20pt">References</h2>'))
        main_page = main_page.parent().parent().parent().parent();
        main_page.children().remove()
        var last_main_page = $(document.createElement("div"))
        last_main_page.addClass("page")
        $("#references_section").remove($("#references_section").children())
        main_page.append(last_main_page)
        var last_sub_page = $(document.createElement("div"))
        last_sub_page.addClass("subpage")
        last_sub_page.append(subpage_header);
        var last_sub_page_content = $(document.createElement("div"))
        last_sub_page_content.addClass("subpage_content")
        last_sub_page.append(last_sub_page_content);
        last_main_page.append(last_sub_page);
        window.total_height = last_sub_page.outerHeight()
        while (children.length > 0) {
            element = children.shift()
            last_sub_page_content.append(element)
            window.total_height += element.outerHeight();
            console.log(window.total_height)
            if (pxTomm(last_sub_page_content.height()) > subpage_height_mm) {
                last_sub_page_content.remove(element)
                children.unshift(element)
                last_main_page = $(document.createElement("div"))
                last_main_page.addClass("page")
                main_page.append(last_main_page)
                last_sub_page = $(document.createElement("div"))
                last_sub_page.addClass("subpage")
                last_sub_page.append(subpage_header)
                var last_sub_page_content = $(document.createElement("div"))
                last_sub_page_content.addClass("subpage_content")
                last_sub_page.append(last_sub_page_content)
                last_main_page.append(last_sub_page);
                window.total_height = last_sub_page.outerHeight()
            }
        }
        real_page_index--;
        $("#References").children().each(function (index) {
            if ($(this).hasClass("page")) {
                $(this).children().append($("<div style='position: absolute; left:48%; bottom:-2cm;'><p style='text-align:center;'>" + (real_page_index++) + "</p></div>"));
            }
        });
        // $('#References').html("<ol style='padding: 0px;'>" + $('#References').html() + "</ol>")
    }
</script>