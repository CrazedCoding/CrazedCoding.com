<!doctype html>
<meta charset=utf-8>
<link rel="stylesheet" href="./css/highlight-syntax.min.css">
<script src="./js/highlight-syntax.min.js"></script>
<script src="./js/highlight-python.min.js"></script>
<style>
    body {
        width: 100%;
        height: 100%;
        margin: 0;
        padding: 0;
        background-color: #fff;
        font-size: 12pt;
        font-family: "Times New Roman", serif
    }

    * {
        box-sizing: border-box;
        -moz-box-sizing: border-box;
    }

    .page {
        width: 210mm;
        min-height: 297mm;
        padding: 20mm;
        margin: 10mm auto;
        border: 1px #fff solid;
        border-radius: 5px;
        background: #fff;
        box-shadow: 0 0 5px rgba(0, 0, 0, .1)
    }

    .subpage {
        padding: 1cm;
        border: 5px #fff solid;
        height: 257mm;
        outline: 2cm #fff solid;
        position: relative;
    }

    .toc_chapter {
        text-align: center;
    }

    .toc_section,
    .toc_subsection {
        text-align: left;
    }

    p {
        text-align: left;
    }

    a {
        color: #33f;
        text-decoration: none;
    }
    code {
        text-align: left;
        font-size: 10pt;
    }

    @page {
        size: A4;
        margin: 0
    }

    @media print {

        body,
        html {
            width: 210mm;
            height: 297mm
        }

        .page {
            margin: 0;
            border: initial;
            border-radius: initial;
            width: initial;
            min-height: initial;
            box-shadow: initial;
            background: initial;
            page-break-after: always
        }
    }
</style>
<script src=./js/jquery.min.js></script>
<script src=./js/bibtex.js></script>

<textarea id="bibtex_input" style="display:none;">
    @online{Medium2018,
        author = "Elvis",
        title = "Elvis, Copyright 2020 A Medium Corporation. \textit{State of the art Multimodal Sentiment Classification in Videos}",
        url  = "https://medium.com/dair-ai/state-of-the-art-multimodal-sentiment-classification-in-videos-1daa8a481c5a",
        addendum = "Accessed on: May 17, 2020.",
    }
    @article{Pouyanfar2017,
        author = {Pouyanfar, Samira and Chen, Shu-Ching},
        year = {2017},
        month = {03},
        pages = {85-109},
        title = {Automatic Video Event Detection for Imbalance Data Using Enhanced Ensemble Deep Learning},
        volume = {11},
        journal = {International Journal of Semantic Computing},
        doi = {10.1142/S1793351X17400050}
    }

    @INPROCEEDINGS{Yang2015,
        author={Y. {Yang} and S. {Chen}},
        booktitle={2015 IEEE International Conference on Information Reuse and Integration}, 
        title={Ensemble Learning from Imbalanced Data Set for Video Event Detection}, 
        year={2015},
        volume={},
        number={},
        pages={82-89},
    }
    @inproceedings{Wan2014,
        author = {W., Ji and Wang, Dayong and Hoi, Steven Chu Hong and Wu, Pengcheng and Zhu, Jianke and Zhang, Yongdong and Li, Jintao},
        title = {Deep Learning for Content-Based Image Retrieval: A Comprehensive Study},
        year = {2014},
        isbn = {9781450330633},
        publisher = {Association for Computing Machinery},
        address = {New York, NY, USA},
        url = {https://doi.org/10.1145/2647868.2654968},
        doi = {10.1145/2647868.2654968},
        booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
        pages = {157–166},
        numpages = {10},
        keywords = {feature representation, content-based image retrieval, convolutional neural networks, deep learning},
        location = {Orlando, Florida, USA},
        series = {MM ’14}
    }
    @misc{Lin2013,
        title={Network In Network},
        author={Min Lin and Qiang Chen and Shuicheng Yan},
        year={2013},
        eprint={1312.4400},
        archivePrefix={arXiv},
        primaryClass={cs.NE}
    }
    @ARTICLE{Chen2009,
        author={X. {Chen} and C. {Zhang} and S. {Chen} and S. {Rubin}},
        journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
        title={A Human-Centered Multiple Instance Learning Framework for Semantic Video Retrieval}, 
        year={2009},
        volume={39},
        number={2},
        pages={228-233},
    }

    @INPROCEEDINGS{Kolekar2008,
        author={M. H. {Kolekar} and K. {Palaniappan} and S. {Sengupta}},
        booktitle={2008 Sixth Indian Conference on Computer Vision, Graphics   Image Processing}, 
        title={Semantic Event Detection and Classification in Cricket Video Sequence}, 
        year={2008},
        volume={},
        number={},
        pages={382-389},
    }

    @INPROCEEDINGS{Lin2007,
        author={L. {Lin} and G. {Ravitz} and M. {Shyu} and S. {Chen}},
        booktitle={2007 IEEE International Conference on Multimedia and Expo}, 
        title={Video Semantic Concept Discovery using Multimodal-Based Association Classification}, 
        year={2007},
        volume={},
        number={},
        pages={859-862},
    }

    @INPROCEEDINGS{Chen2007,
        author={M. {Chen} and C. {Zhang} and S. {Chen}},
        booktitle={International Conference on Semantic Computing (ICSC 2007)}, 
        title={Semantic Event Extraction Using Neural Network Ensembles}, 
        year={2007},
        volume={},
        number={},
        pages={575-580},
    }


    @ARTICLE{Lecun1998,
        author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
        journal={Proceedings of the IEEE}, 
        title={Gradient-based learning applied to document recognition}, 
        year={1998},
        volume={86},
        number={11},
        pages={2278-2324},
    }
</textarea>
<div id=100_mm_ruler style=height:100mm;display:none></div>
<div class=page>
    <div class=subpage>
        <br>
        <p style=text-align:center;font-size:26pt><strong>A Novel Multimodal Model for<br>Event Detection in
                Videos</strong></p>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <p style=text-align:center;font-size:18pt>Ian Logan Wesson
            <br>(安龙)</p>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <p style=text-align:center;font-size:18pt>June 2020
    </div>
</div>
<div class=page>
    <div class=subpage>
        <br>
        <p style=text-align:center;font-size:26pt><strong>A Novel Multimodal Model for<br>Event Detection in
                Videos</strong></p>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <p style=text-indent:1.4in;font-size:14pt>Candidate: <u> Ian Logan Wesson 安龙 </u></p>
        <p style=text-indent:1.4in;font-size:14pt>School of Study: <u> School of Software </u></p>
        <p style=text-indent:1.4in;font-size:14pt>Student’s Number: <u> 3820181125 </u></p>
        <p style=text-indent:1.4in;font-size:14pt>Supervisor: <u> 张华平 Hua Ping Zhang </u></p>
        <p style=text-indent:1.4in;font-size:14pt>Chair of Defense Committee: <u> ? </u></p>
        <p style=text-indent:1.4in;font-size:14pt>Degree Applied: <u> Master of Computer
                Science </u></p>
        <p style=text-indent:1.4in;font-size:14pt>Major: <u> Computer Science and
                Technology </u></p>
        <p style=text-indent:1.4in;font-size:14pt>Date of Defense: <u> June 2020 </u></p>
        <p style='text-align:center;font-family:"Times New Roman",serif;margin-bottom:0;line-height:100%'>
            <br></p>
    </div>
</div>
<div class=page>
    <div class=subpage>
        <p style=text-align:center;margin-bottom:0;line-height:150%;font-size:16pt><strong>Declaration of
                Originality</strong></p>
        <p style=margin-bottom:0;line-height:150%;text-align:justify>
            <br></p>
        <p style=margin-bottom:0;line-height:150%;text-align:justify>I hereby declare that this thesis is
            the result of an independent research that I have made under the supervision of my supervisor.
            To the best of my knowledge and belief, it does not contain any other published or unpublished
            research work of others, or any material which has been accepted for the award of another degree
            or diploma at Beijing Institute of Technology or other educational institutions, except where
            due acknowledgment has been made in the text. Whoever has contributed to this study is
            explicitly identified and appreciated in the Acknowledgements of the thesis.</p>
        <p style=margin-bottom:0;line-height:150%;text-align:justify>
            <br></p>
        <p style=margin-bottom:0;line-height:125%;text-align:justify>Signature of
            Author: <u>      </u></p>
        <p style=margin-bottom:0;line-height:125%;text-align:justify>Date: <u>      </u></p>
        <p style=margin-bottom:0;line-height:150%;text-align:justify>
            <br></p>
        <p style=margin-bottom:0;line-height:150%;text-align:justify>
            <br></p>
        <p style=margin-bottom:0;line-height:150%;text-align:justify>
            <br></p>
        <p style=text-align:center;margin-bottom:0;line-height:150%;font-size:16pt>
            <strong>Authorization Statement</strong>
            <p style=margin-bottom:0;line-height:150%;text-align:justify>
                <br></p>
            <p style=margin-bottom:0;line-height:150%;text-align:justify>I
                fully understand the regulations of Beijing Institute of
                Technology regarding the preservation and use of the degree
                thesis. BIT is granted the right to (1) preserve the
                original thesis and the copies of the <em class=western><span
                        style=font-style:normal>thesis</span></em>, as well
                as submit them to relevant authorities; (2) reproduce and
                preserve the thesis by means of photocopy, reduction
                printing and any other means; (3) keep the thesis for
                reference and borrowing; (4) reproduce, give and exchange
                the thesis for the purpose of academic exchange; (5) publish
                the <em class=western><span style=font-style:normal>thesis
                        wholly and partially</span></em>. (The regulations
                comes into force for confidential thesis only after
                declassification)</p>
            <p style=margin-bottom:0;line-height:150%;text-align:justify>
                <br></p>
            <p style=margin-bottom:0;line-height:150%;text-align:justify>
                Student’s signature： <u>      </u>   Date:
                 <u>      </u></p>
            <p style=margin-bottom:0;line-height:150%;text-align:justify>
                Supervisor’s signature： <u>      </u>   Date:
                 <u>      </u></p>
    </div>
</div>
<toc_frontmatter_entry class=toc_frontmatter_entry id=Acknowledgments>
    <p style=line-height:150%>  Thank you Prof. ... and Rep. of China for this opportunity. I hope these advances
        will be applied.</p>
</toc_frontmatter_entry>
<toc_frontmatter_entry class=toc_frontmatter_entry id=Abstract>
    <p style=line-height:150%>   Querying videos for many types of information, from cats and dogs, to instances of
        crime, abuse, violence or terrorism, in a way that returns a frame-by-frame analysis is becoming more and
        more important with the rapid expansion of online videos, especially for social media sites. Although there
        is some research into video classification, event detection in video analysis is still largely
        underdeveloped. For starters, even readily available open- source research into video event detection does
        not use a multimodal approach. Moreover, natural language processing and linguistic modalities are often not
        even used as a
        way of detecting events in any video analysis software. Last but not least, all frame-by-frame solutions to
        video analysis are proprietary or closed-source.</p>
    <p style=line-height:150%>   Compared to other modern video analysis tools and techniques, the proposed
        framework: (1) will implement a novel multimodal model for detecting events; (2) will utilize existing
        natural language processing software as an intermediate processing layer; and (3) will not be
        proprietary or closed source, but instead be built from entirely open-source software and hosted on a
        publicly accessible source code revision control platform (GitHub). The proposed framework has changed
        since there has been time to develop it, as many different tools and methodologies were briefly tested
        as a robust prototype system evolved over the duration of the research (and this discussed in
        detail at the beginning of ??).</p>
    <p style=line-height:150%>    When a set of input videos are given, the output of the novel model is the
        probability that a given query event string occurs in each of the input videos frames. The videos
        can then be sorted frame-by-frame according to the likelihood of them depicting that particular
        query event. This will allow for more effective content monitoring, as well as more effective
        content browsing in general.</p>
    <div style=position:relative><span
            style="font-style:normal;font-weight:700;font-size:12pt;font-family:Times New Roman;color:#000">Key
            Words: </span><span
            style="font-style:normal;font-weight:400;font-size:12pt;font-family:Times New Roman;color:#000">Multimodal;
            Neural Network; Machine Learning; Computer Vision; Natural Language Processing; Audio Signal
            Processing; Time Series Prediction;</span></div>
</toc_frontmatter_entry>
<toc_frontmatter_entry id="Table of Contents" class="toc_frontmatter_entry table_of_contents">
    <div id=toc_list style="list-style-type:none;"></div>
</toc_frontmatter_entry>
<toc_chapter class=toc_chapter id=Introduction>
    <p style=line-height:150%>    The proposed framework was designed around a novel multimodal model which would
        process the output of the following open-source NodeJS software solutions: automatic speech recognition
        (using the deepspeech npm library), object detection (using the opencv4nodejs or tensorflowjs npmlibraries),
        and natural language processing (using the word2vector npm library).</p>
    <p style=line-height:150%>   The main objective of the work proposed was finding the best combinations of
        libraries, programming tools, and even operating systems, suitable for creating an efficient and
        innovative neural network applications research and development environment. During this extensive
        search, different API’s, such as OpenCV, PyTorch, and TensorFlow, in various languages, including NPM/NodeJS,
        JavaScript, and Python, were tested on the Windows 10, Ubuntu 19.04, and Linux Mint 19.3 operating
        systems.</p>
    <p style=line-height:150%>   After testing these combinations, it was found that Linux Mint with Pytorch
        and TensorFlow/Keras in Python 3.6.9 was the best combination of software technologies for reliably
        running experiments on our particular hardware setup.</p>
    <p style=line-height:150%>   As such, all of of the code segments, final project code, and diagrams
        of this research project are for Pytorch and TensorFlow/Keras in Python 3.6.9. You can find an
        online copy of the final project code on GitHub. The project link you should refer to when
        reading this document is a link is to a specific branch named ”thesis” of a GitHub repository
        (which will host future developments on the ”master” branch):
        https://github.com/CrazedCoding/CrazedCoding.com/tree/thesis</p>
    <p style=line-height:150%>   One of the accomplishments of this research includes a multimodal
        neural network model based system that allows users to process videos frame-by-frame for
        images with content of varying degrees of similarity to query string parameters. The research on this topic
        covered in 
        <toc_reference name="Visual Modalities"></toc_reference> and the experiments are in
        covered in detail in <toc_reference name="Visual Analysis - Video Content Searching with Query Strings"></toc_reference>
        </p>
    <p style=line-height:150%>   The second accomplishment of this
        research was the creation of a command-line tool which enabled the user to train a
        multi-layer neural network to learn and predict audio samples. The research on this topic
        covered in 
        <toc_reference name="Auditory Modalities"></toc_reference> and the experiments are in
        <toc_reference name="Audio Analysis - Signal Frequency Domain Prediction"></toc_reference>.
    </p>
    <p>Introduction should be 3 Sections: (1.1) background (why you do this work?)
        (1.2) previous work (what have others done (briefly)? what are the advantages/disadvantages?)
        (1.3) paper structure (how is this paper organized?)
        <br>
        Put table captions above table.
        <br>
        Replace table/figure/code section
        <br>
        Existing Techniques -> "Works Related to Video Event Detection"
        <br>
        Change subsections of chapter 2 (longer):
        2.1 - Event Detection on Videos 
        2.2 - Deep Learning
        2.3 - Audio Signal Analysis
        2.4 - Computer Vision
        2.5 - NLP
        2.6 - Multimodal Models
        <br>
        After chapter 2 you can only use your own work.
        <br>
        Multimodal Model Architectures for Event Detection
        3.1 Architecture design (include table 4.1a and 4.2a should go here).
            Explain relationship to other architectures.
            What are the advantages of this architecture.
        <br>
        4.1 -> new chapter 4 (include "experiment" in name)
        4.1.1 -> How-To
        4.2 -> new chapter 5 (include "experiment" in name)
        <br>
        Add new chapter 6 -> "System Implementation"
            (1) System name.
            (2) Copy GitHub page ("Server Installation and Configuration")
            (3) Screenshots
        <br>
        Remove chapter number from conclusion/references.
        <br>
        Add final section "Future Work"
            - We ran out of time.
        <br>
        Add "Chapter" to chapter numbers in TOC.
        <br>
        Fix bibtex reference to be like Yvette's paper.
        <br>
    </p>
</toc_chapter>
<toc_chapter class=toc_chapter id="Existing Techniques">
    <p style=line-height:150%>    The following three sections provide a detailed overview of the state-of the art
        research into event detection in videos.</p>
    <toc_section class=toc_section id="Semantic Event Detection Using Ensemble Deep Learning">
    </toc_section>
    <p style=line-height:150%><a class="bibtex_reference" bibtexkey="Pouyanfar2017"></a> has the following to say about
        semantic event
        detection:</p>
    <p style="line-height:150%; margin-left:7.5%;">Recently, many researchers have tried to detect the most interesting
        events and concepts from
        videos (<a class="bibtex_reference" bibtexkey="Lin2007"></a>, <a class="bibtex_reference"
            bibtexkey="Chen2009"></a>). Criminal event detection from video and audio data, natural disaster retrieval
        from video
        data, and interesting event detection in a sport game are a few examples of video semantic event detection.</p>
    <p style=line-height:150%><a class="bibtex_reference" bibtexkey="Pouyanfar2017"></a> then goes into detail about it’
        s proposed ensemble deep learning framework:</p>
    <p style="line-height:150%; margin-left:7.5%;">Deep learning is not a new topic and has a long history in artificial
        intelligence (<a class="bibtex_reference" bibtexkey="Wan2014"></a>). Convolutional Neural Networks (CNNs) <a
            class="bibtex_reference" bibtexkey="Lecun1998"></a>, for instance, have improved traditional
        feedforward neural networks in 1990s, especially in image processing, by constraining the complexity of
        networks using local weight sharing topology. Traditional neural network techniques are difficult to
        interpret due to their black-box nature and they are also very prone to over-fitting <a class="bibtex_reference"
            bibtexkey="Yang2015"></a>. In contrast,
        new deep learning algorithms are more interpretable because of their strong local modeling. In addition,
        as new ideas, algorithms, and network architectures have been designed in the last few years, deep
        learning has shown significant advances mainly in image recognition and object detection.</p>
    <p style="line-height:150%; margin-left:7.5%;"> As a single classifier may not be able to handle large datasets with
        multiple feature sources, ensemble algorithms have attracted lots of attention in the literature,
        which can be utilized to enhance the classification performance by taking advantages of multiple
        classifiers. A positive enhanced ensemble algorithm which handles imbalanced data in video event
        retrieval is presented <a class="bibtex_reference" bibtexkey="Chen2007"></a>. Their proposed framework combines
        a sampling-based method with a
        classifier fusion algorithm to enhance the detection of interesting events (minor classes) in an
        imbalanced sport video dataset. An ensemble neural network is proposed in <a class="bibtex_reference"
            bibtexkey="Kolekar2008"></a>. </p>
    <toc_figure id="figure1" description="A proposed ensemble deep learning framework.">
        <img src=./imgs/figure1.png style=max-width:100%>
    </toc_figure>
    <p style="line-height:150%; margin-left:7.5%;">Using a bootstrapped
        sampling approach along with a group of neural networks, the rare event issue is alleviated. The
        framework was also evaluated using a large set of soccer videos with the purpose of corner event
        detection.</p>
    <p style=line-height:150%>    <a class="bibtex_reference" bibtexkey="Pouyanfar2017"></a>’s method is divided into
        three main modules: (1) preprocessing, (2) deep feature
        extraction, and (3) classification including training, validation, and testing. <a class="bibtex_reference"
            bibtexkey="Pouyanfar2017"></a> concludes
        that: “the experimental results demonstrate the effectiveness of the proposed framework for
        video event detection.”
        detection.</p>
    <page_break></page_break>
    <toc_section class=toc_section id="Semantic Event Detection and Classification in Cricket Video Sequences">
    </toc_section>
    <p style=line-height:150%>    <a class="bibtex_reference" bibtexkey="Kolekar2008"></a> provides a good
        classification of video sequences. It is included here as a potential solution for creating training
        mechanisms for deep learning models. </p>
        <p style="line-height:150%; margin-left:7.5%";>    In this paper, video semantic analysis is formulated
        based on low-level image features and high-level knowledge. The
        sports domain semantic knowledge encoded in the hierarchical classification not only reduces
        the cost of processing data drastically, but also significantly increases the classifier accuracy.
        The hierarchical framework enables the use of simple features and organizes the set of features
        in a semantically meaningful way. The proposed hierarchical semantic framework for
        event classification can be readily generalized to other sports domains as well as other types
        of video.</p>
    <toc_section class=toc_section id="State of the Art Multimodal Sentiment Classication in Videos">
    </toc_section>
    <p style=line-height:150%>    According to <a class="bibtex_reference" bibtexkey="Medium2018"></a>, there are
        basically two frameworks for fusing modalities:</p>
    <p style="line-height:150%; margin-left:7.5%">   
        Non-hierarchical Frameworks where unimodal features are concatenated and fed into
        the various contextual LSTM networks proposed above (e.g., h-LSTM), and </p>
    <p style="line-height:150%; margin-left:7.5%">   Hierarchical Frameworks
        where the difference here is that we don’ t concatenate unimodal features, we feed each
        unimodal feature into the LSTM network proposed above.</p>
    <p style=line-height:150%>    Think of this framework as having
        some hierarchy. In the first level, unimodal features are fed individually to LSTM networks.
        The output of the first level are then concatenated and fed into another LSTM network (i.e.,
        second level). <a class="bibtex_reference" bibtexkey="Medium2018"></a> provides the reader with an essential
        overview of it’s proposed model.
        It is as follows:</p>
    <p style=line-height:150%>    In testing the proposed model, <a class="bibtex_reference" bibtexkey="Medium2018"></a>
        gives the following findings: </p>
        <p style="line-height:150%; text-indent:7.5%">1. They observed
        that hierarchical model significantly outperform the non-hierarchical frameworks. </p>
        <p style="line-height:150%; text-indent:7.5%">2. They
        observed that sc-LSTM and bc-LSTM models perform the best out of the LSTM variants, including
        the uni-SVM model. These results helped to show the importance of considering contextual
        information when classifying utterances. </p>
        <p style="line-height:150%; text-indent:7.5%">3. In general, unimodal classifiers trained on
        textual information performed better compared to other individual modalities (results highlighted
        in blue). Combining the modalities tended to boost the performance, indicating that multimodal
        methods are feasible and effective. </p>
        <p style="line-height:150%; text-indent:7.5%">4. Individually, the visual modality caries
        more generalized information. Overall, fusing the modalities improved the model.</p>
    <p style=line-height:150%>    For reference, the diagram demonstrating the multimodal model
        from <a class="bibtex_reference" bibtexkey="Medium2018"></a> is included in 
        <toc_reference name="figure2"></toc_reference> below. </p>
    <toc_figure id="figure2" description="The overview of another proposed framework.">
        <img src=./imgs/figure2.png style=max-width:85%>
    </toc_figure>
</toc_chapter>
<toc_chapter class=toc_chapter id="Multimodal Neural Networks (Research)">
    <p style=line-height:150%>    To create a multimodal neural network we had to ascertain knowledge about a few
        different modes, particularly the auditory, visual, and linguistic modalities. The following chapter
        provides a detailed explanation of the research into these different modes. For reference,
        the multimodal model system that was originally proposed is included below in <toc_reference name="figure3"></toc_reference>
        below:</p>
    <toc_figure id="figure3" description="Overview of the originally proposed framework.">
        <img src=./imgs/figure3.png style=max-width:80%>
    </toc_figure>
    <p style=line-height:150%>    Together, <toc_reference name="figure3"></toc_reference>,
        in combination with <toc_reference name="figure4"></toc_reference> and <toc_reference name="figure5"></toc_reference> 
        (in the following chapters where the
        final results are discussed) make it easy to discern the evolution of the research project’s
        methodologies from start to finish as more was learned about the actual capabilities of the
        software and hardware available.</p>
    <p style=line-height:150%>    The original time table for our neural network experiments was designed to test three
        hypotheses to three questions which are listed in <toc_reference name="table1"></toc_reference> below, along with what our research and
        experiments indicated about the answers.</p>
    <toc_table id="table1" description="Hyptheses, questions, and answers of the research project.">
        <table style="border-collapse:collapse;border-spacing:0" class="tg">
            <thead>
                <tr>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Questions</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Hypotheses</th>
                    <th
                        style="border-color:inherit;border-style:solid;border-width:1px; overflow:hidden;padding:10px 5px; word-break:normal">
                        Answers</th>
                </tr>
            </thead>
            <tbody style="text-align:left;">
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Is it possible to use entirely open-source
                            technology to build the framework of a video based event detection algorithm?</span><br>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Yes, because there are many open source
                            projects
                            for the basic requirements of such a framework.</span></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Moderately supported by research described in
                            sections <toc_reference name="Automatic Speech Recognition"></toc_reference>. Not supported by experiments at all (due to noise).</span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Is there a multimodal model for fast and
                            accurate
                            event detection on video media?</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Yes. This hypothesis is based on the fact
                            current
                            research is capable of detecting anomalous and suspicious activity in images.</span><br>
                    </td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Highly supported by all of the reserch and in this chapter's research, and the experiments described in <toc_reference name="Analyzing Audio and Video (Experiments)"></toc_reference>,
                            particularly the one described in <toc_reference name="Visual Analysis - Video Content Searching with Query Strings"></toc_reference>.</span><br></td>
                </tr>
                <tr>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Can multimodal models improve the performance
                            of
                            content moderators?</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">Yes, because video content can be more
                            accurately
                            and precisely analyzed for events.</span><br></td>
                    <td
                        style="border-color:inherit;border-style:solid;border-width:1px; font-size:11pt;overflow:hidden;padding:10px 5px; word-break:normal">
                        <span style="font-weight:normal;font-style:normal">This is highly supported by the video processing research and experiments
                            , and the moderator’s point of view is demonstrated in
                            <toc_reference name="How-To Usage Guide of the Multimodal System"></toc_reference></span><br></td>
                </tr>
            </tbody>
        </table>
    </toc_table>
    <p style=line-height:150%>    Most of our hypotheses were strongly supported by the results the experiments
        (described in chapter 4), with one notable exception to the first question and hypothesis: our
        research indicated that even though there are many open-source software solutions for automatic
        speech recognition (ASR), virtually all of them suffer from severe inaccuracy when
        transcribing audio without proper recording conditions or denoising techniques. The decision
        was made to abandon ASR as a solution for creating one of the modes for the multimodal network.</p>
    <p style=line-height:150%>    The pitfall of losing the preferred choice of auditory mode (spoken words from audio
        signals transcribed as text) made the process of creating a multimodal system difficult because
        our only other preferred choices of modes were going to be the visual and spacial modes of a
        video stream.</p>
    <p style=line-height:150%>    In addition to the aforementioned pitfall, there was another major pitfall: the
        variety
        of information available as output from pretrained object detection algorithms (which we
        planned to use for the visual and spacial modes of our multimodal system) was very limited:
        most pretrained object detection models for desktop environments can produce a maximum
        of around 10 visual classes (the types of the objects detected), which, even in combination
        with spacial output mode of object detection algorithms (the positions of the objects detected),
        could not fulfil the requirement of producing a search space out of the input videos (which
        had a variety of types of content) accurate, precise, or even organized enough to query for
        generalized user search string parameters.</p>
    <p style=line-height:150%>    The two aforementioned pitfalls meant that we could not use ASR or object detection
        to create any of the input modes for our multimodal system as we had planned to initially.
        To meet time requirements, image classification models would instead be used for the visual
        mode of the system, which could output around 1000 different output classes per image.
        Additionally, natural language processing (NLP) word embeddings would be used to create
        a linguistic mode of input for our multimodal system.</p>
    <p style=line-height:150%>    Even though using these two modes of input (visual and linguistic) for our system
        would
        meet the initial requirement of creating a multimodal system, doing just this would have
        meant the final product of our video-processing research would entirely ignore the audio. The
        research project was initially designed to include audio signal analysis (in the form of ASR)
        to create a complete understanding of applying multimodal neural networks in the context
        visual and auditory analysis of videos, so following the conclusion of our research and experiments with
        video processing (<toc_reference name="Visual Analysis - Video Content Searching with Query Strings"></toc_reference>
        and <toc_reference name="Visual Analysis - Video Content Searching with Query Strings"></toc_reference>,
        respectively) we abruptly switched to audio signal analysis research and
        experiments 
        (<toc_reference name="Visual Analysis - Video Content Searching with Query Strings"></toc_reference>
        and <toc_reference name="Visual Analysis - Video Content Searching with Query Strings"></toc_reference>,
        respectively). The audio signal experiment comes in the form of a point-by-point signal frequency
        prediction Python 3.6.9 program.</p>
    <page_break></page_break>
    <toc_section class=toc_section id="Visual Modalities">
    </toc_section>
    <p style=line-height:150%>    The following code segment demonstrates the most important process we found for
        loading videos into memory. Our first approach was to use Pytorch’s torchvision.io module, but
        we later opted for the SciPy ffmpeg-python module to process a video files frame-by-frame
        using numpy because of it’s speed, versatility, and compatibility with Pytorch. You can find
        out more about how we used this functionality by referencing ”audio_processing.py” in the
        root folder of the GitHub branch accompanying this document.</p>

    <toc_code_segment id="code1" description="Overview of the video loading code.">
        <pre><code class="python hljs">
import ffmpeg
import subprocess
import sys
import numpy as np

process1 = (
    ffmpeg
    .input(in_filename)
    .output('pipe:', format='rawvideo', pix_fmt='rgb24')
    .run_async(pipe_stdout=True)
)
process2 = (
    ffmpeg
    .input('pipe:', format='rawvideo', pix_fmt='rgb24',\\
    s='{}x{}'.format(width, height))
    .output(out_filename, pix_fmt='yuv420p')
    .overwrite_output()
    .run_async(pipe_stdin=True)
)
while True:
    in_bytes = process1.stdout.read(width * height * 3)
    if not in_bytes:
        break
    in_frame = (
        np
        .frombuffer(in_bytes, np.uint8)
        .reshape([height, width, 3])
    )
    out_frame = in_frame * 0.3
    process2.stdin.write(
        frame
        .astype(np.uint8)
        .tobytes()
    )
process2.stdin.close()
process1.wait()
process2.wait()
        </code></pre>
    </toc_code_segment>
    <p style=line-height:150%>    The following two subsections are meant to describe the research items that were most
        useful for our experiments in the context of visual modes when analyzing video files using
        neural networks.</p>
    <toc_subsection class=toc_subsection id="Image Classification">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Object Detection (Spacial Modality)">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_section class=toc_section id="Auditory Modalities">
    </toc_section>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Automatic Speech Recognition">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Sound Classifications">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_section class=toc_section id="Linguistic Modalities">
    </toc_section>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Word Embeddings (NLP)">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
</toc_chapter>
<toc_chapter class=toc_chapter id="Analyzing Audio and Video (Experiments)">
    <toc_section class=toc_section id="Visual Analysis - Video Content Searching with Query Strings">
    </toc_section>
    <p style=line-height:150%>    The system in the following diagram is a representation the client-server application
        we created which combines the modalities of the frame images (from user-provided videos) with the their
        word embeddings (or multi-dimensional meaning representations of a word). This creates the
        desired ability for the end user to query complex concepts via the linguistic modality (rather
        than a series of individual image classifications) quickly and efficiently, with ever improving
        accuracy and precision with each refinement of a query.</p>
    <toc_figure id="figure4" description="Overview of the final framework.">
        <img src="./imgs/figure4.png" style=max-width:85%>
    </toc_figure>
    <page_break></page_break>
    <p style=line-height:150%>    Because the entirety of a user’s video content is pre-processed and stored in word
        vector
        embeddings upon uploading, binary search procedures of the entire domain of a user’s uploaded video content can
        be strategically reorganized categorically with rapid efficiency by
        the server system, and with very little effort by the end-user. This multimodal functionality
        ran with incredible efficiently on the prototype system.</p>
    <p style=line-height:150%>    Develop an event detection algorithm prototype for demonstrating the concept of a mul-
        timodal event detection process. Describe the system (heavy references to chapter 3), what
        did you do?</p>
    <toc_subsection class=toc_subsection id="How-To Usage Guide of the Multimodal System">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Performance Analysis of the Multimodal System">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Scalability Analysis of the Multimodal System">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Comparison Between Existing Multimodal Systems">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_section class=toc_section id="Audio Analysis - Signal Frequency Domain Prediction">
    </toc_section>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_figure id="figure5" description="Overview of an additional final product.">
        <img src=./imgs/figure5.png style=max-width:100%>
    </toc_figure>
    <toc_subsection class=toc_subsection id="How-To Usage Guide of the Predction System.">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Performance Analysis of the Predction System">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Scalability Analysis of the Predction System">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <toc_subsection class=toc_subsection id="Comparison Between Existing Predction Systems">
    </toc_subsection>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
    <p style=line-height:150%>   ............... ...
       ... ...
        ... ...</p>
</toc_chapter>
<toc_chapter class=toc_chapter id="Conclusion">
</toc_chapter>
<toc_chapter class=toc_chapter id="References">
    <div class="bibtex_display" callback="window.bibtex_callback(bibtex_display)" style="text-align: left;"></div>
</toc_chapter>
<script>
    var subpage_height_mm = 217;
    var pxTomm = function (px) {
        return Math.floor(px / ($('#100_mm_ruler').height() / 100)); //JQuery returns sizes in PX
    };
    var subpage_header = `
                <div style="position: relative; top:-1cm; margin-bottom:-1cm; text-align: center;"><span style="font-style:normal;font-weight:normal;font-size:10pt;font-family:Times New Roman;color:#000000">Beijing Institute of Technology Master Degree Thesis</span><span style="font-style:normal;font-weight:normal;font-size:10pt;font-family:Times New Roman;color:#000000"> </span><br/></SPAN></div>
                <img style="position: relative; top:-1.35cm; margin-bottom:-1cm; text-align: center;" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAk8AAAACCAYAAABBnjj1AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAAfSURBVFhH7cOxDQAADAIg/3/anmC6Q0IAAPirqqprek3dS9EBwTMwAAAAAElFTkSuQmCC" />
                <br>
                `

    function romanize(num) {
        if (isNaN(num)) return NaN;
        var digits = String(+num).split(""),
            key = ["", "C", "CC", "CCC", "CD", "D", "DC", "DCC", "DCCC", "CM", "", "X", "XX", "XXX", "XL", "L", "LX", "LXX", "LXXX", "XC", "", "I", "II", "III", "IV", "V", "VI", "VII", "VIII", "IX"],
            roman = "",
            i = 3;
        while (i--) roman = (key[+digits.pop() + (i * 10)] || "") + roman;
        return Array(+digits.join("") + 1).join("M") + roman;
    }

    window.onload = function () {
        var toc_fm_pages = []
        var toc_fm_header_strings = []
        var toc_fm_header_elements = []
        $(".toc_frontmatter_entry").each(function (index) {
            var main_page = $(this);
            toc_fm_header_strings.push(main_page.attr('id'))
            var children = []
            main_page.children().each(function (index) {
                children.push($(this));
                main_page.remove($(this))
            });
            var last_main_page = $(document.createElement("div"))
            last_main_page.addClass("page")
            $(this).append(last_main_page)
            var last_sub_page = $(document.createElement("div"))
            last_sub_page.addClass("subpage")
            last_sub_page.append(subpage_header);
            var last_sub_page_content = $(document.createElement("div"))
            last_sub_page_content.addClass("subpage_content")

            var toc_header = $('<h2 style="text-align:center; font-size: 20pt"></h2>')
            toc_fm_header_elements.push(toc_header)
            last_sub_page.append(toc_header)
            last_sub_page.append(last_sub_page_content)
            last_main_page.append(last_sub_page);
            toc_fm_pages.push(last_sub_page)
            window.total_height = last_sub_page.outerHeight()
            while (children.length > 0) {
                element = children.shift()
                last_sub_page_content.append(element)
                window.total_height += element.outerHeight();
                console.log(window.total_height)
                if (pxTomm(last_sub_page_content.height()) > subpage_height_mm) {
                    last_sub_page_content.remove(element)
                    children.unshift(element)
                    last_main_page = $(document.createElement("div"))
                    last_main_page.addClass("page")
                    $(this).append(last_main_page)
                    last_sub_page = $(document.createElement("div"))
                    last_sub_page.addClass("subpage")
                    last_sub_page.append(subpage_header)
                    var last_sub_page_content = $(document.createElement("div"))
                    last_sub_page_content.addClass("subpage_content")
                    last_sub_page.append(last_sub_page_content)

                    last_main_page.append(last_sub_page);
                    toc_fm_pages.push(last_sub_page)
                    window.total_height = last_sub_page.outerHeight()
                }
            }
        });
        var toc_chapters = []
        var toc_chapter_pages = []
        var toc_chapter_names = []
        var toc_chapter_elements = []
        var page_number = 1;
        $(".toc_chapter").each(function (index) {
            var main_page = $(this);
            toc_chapter_names.push(main_page.attr('id'))
            var accumulation = []
            var children = []
            main_page.children().each(function (index) {
                children.push($(this));
                main_page.remove($(this))
            });
            var last_main_page = $(document.createElement("div"))
            last_main_page.addClass("page")
            last_main_page.attr('number', page_number++);
            $(this).append(last_main_page)
            var last_sub_page = $(document.createElement("div"))
            last_sub_page.addClass("subpage")
            last_sub_page.append(subpage_header);
            var last_sub_page_content = $(document.createElement("div"))
            last_sub_page_content.addClass("subpage_content")

            var toc_header = $('<h2 style="text-align:center; font-size: 20pt"></h2>')
            toc_chapter_elements.push(toc_header)
            last_sub_page.append(toc_header)
            last_sub_page.append(last_sub_page_content)
            last_main_page.append(last_sub_page);
            toc_chapters.push(last_sub_page)
            toc_chapter_pages.push(last_sub_page)
            window.total_height = last_sub_page.outerHeight()
            while (children.length > 0) {
                element = children.shift()
                last_sub_page_content.append(element)
                window.total_height += element.outerHeight();
                console.log(window.total_height)
                if (element.is("page_break") || pxTomm(last_sub_page_content.height()) > subpage_height_mm) {
                    last_sub_page_content.remove(element)
                    if (!element.is("page_break"))
                        children.unshift(element)
                    last_main_page = $(document.createElement("div"))
                    last_main_page.addClass("page")
                    last_main_page.attr('number', page_number++);
                    $(this).append(last_main_page)
                    last_sub_page = $(document.createElement("div"))
                    last_sub_page.addClass("subpage")
                    last_sub_page.append(subpage_header)
                    var last_sub_page_content = $(document.createElement("div"))
                    last_sub_page_content.addClass("subpage_content")
                    last_sub_page.append(last_sub_page_content)

                    last_main_page.append(last_sub_page);
                    toc_chapter_pages.push(last_sub_page)
                    window.total_height = last_sub_page.outerHeight()
                }
            }
        });
        var toc_names = []
        var toc_depths = []
        var depth = 0;
        for (var i = 0; i < toc_fm_header_elements.length; i++) {
            toc_fm_header_elements[i].text(toc_fm_header_strings[i])
        }
        for (var i = 0; i < toc_chapter_elements.length; i++) {
            toc_chapter_elements[i].text("Chapter " + (i + 1) + " - " + toc_chapter_names[i])
        }

        var toc_list = $("#toc_list");
        var dots = '........................................................................................................................';

        for (var i = 0; i < toc_fm_pages.length; i++) {
            toc_list.append($('<div><div style="float:left; display:inline-block; overflow: hidden; white-space: nowrap;max-width:92.5%;text-overflow:ellipsis;"><p><a href="#' + toc_fm_header_strings[i] + '">' + toc_fm_header_strings[i] + "</a>" + dots + '</p></div><div style="min-width:5%;text-align:center;display:inline-block;"><p  style="text-align:center;">' + romanize(i + 1) + '</p></div></div>'));
        }

        var chapter = 0;
        var section = 0;
        var subsection = 0;
        var figure = 0;
        var table = 0;
        var code_segment = 0;

        var is_chapter = false;
        var is_section = false;
        var is_subsection = false;

        $(".toc_chapter, .toc_section, .toc_subsection, toc_figure, toc_table, toc_code_segment").each(function (index) {
            if ($(this).hasClass("toc_chapter")) {

                is_chapter = true;
                is_section = false;
                is_subsection = false;

                var page = 0
                var parent = $(this)
                var children = parent.children()
                if (children.hasClass("page")) {
                    page = children.attr('number');
                }

                var name = $(this).attr('id');
                chapter += 1;
                section = 0;
                subsection = 0;
                figure = 0;
                table = 0;
                code_segment = 0;
                toc_list.append($(
                    '<div> <div style="float:left; display:inline-block; overflow: hidden; white-space: nowrap;max-width:92.5%;text-overflow:ellipsis;">' +
                    '<p><a href="#' + name + '">' + chapter + '.&ensp;' + name + "</a>" + dots + '</p></div>' +
                    '<div style="min-width:5%;text-align:center;display:inline-block;"><p style="text-align:center;">' + page + "</p></div></div>"));
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>chapter " + chapter + "</a>");
                });
            }
            if ($(this).hasClass("toc_section")) {
                is_chapter = false;
                is_section = true;
                is_subsection = false;
                var page = 0
                var parent = $(this)
                while (parent = parent.parent()) {
                    if (parent.hasClass("page")) {
                        page = parent.attr('number');
                        break;
                    }
                }

                var name = $(this).attr('id');
                section += 1;
                subsection = 0;
                figure = 0;
                table = 0;
                code_segment = 0;
                toc_list.append($(
                    '<div><div style="float:left; display:inline-block; overflow: hidden; white-space: nowrap;max-width:92.5%;text-overflow:ellipsis;"><p>' +
                    '&emsp;&emsp;<a href="#' + name + '">' + chapter + "." + section + '&ensp;' + name + '</a>' + dots + '</p></div><div style="min-width:5%;text-align:center;display:inline-block;"><p style="text-align:center;">' +
                    page + "</p></div></div>"));

                $(this).html("<h3>" + chapter + "." + section + " - " + name + "</h3>");
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>section " + chapter + "." + section + "</a>");
                });
            }
            if ($(this).hasClass("toc_subsection")) {
                is_chapter = false;
                is_section = false;
                is_subsection = true;
                var page = 0
                var parent = $(this)
                while (parent = parent.parent()) {
                    if (parent.hasClass("page")) {
                        page = parent.attr('number');
                        break;
                    }
                }
                var name = $(this).attr('id');

                subsection += 1;
                figure = 0;
                table = 0;
                code_segment = 0;
                toc_list.append($(
                    '<div><div style="float:left; display:inline-block; overflow: hidden; white-space: nowrap;max-width:92.5%;text-overflow:ellipsis;"><p>' +
                    '&emsp;&emsp;&emsp;<a href="#' + name + '">' + + chapter + "." + section + "." + subsection + '&ensp;' + name + '</a>' + dots + '</p></div><div style="min-width:5%;text-align:center;display:inline-block;"><p style="text-align:center;">' +
                    page + "</p></div></div>"));

                $(this).html("<h3>" + chapter + "." + section + '.' + subsection + " - " + $(this).attr('id') + "</h3>");
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>subsection " + chapter + "." + section + '.' + subsection + "</a>");
                });
            }
            if ($(this).is("toc_figure")) {

                var name = $(this).attr('id');
                figure += 1;

                $(this).append($("<p style='text-align:center;'><b>Figure "+chapter + (is_section || is_subsection ? "." + section : '') + (is_subsection ? "." + subsection : '') + String.fromCharCode(96 + figure) + "</b>: " + $(this).attr('description') + "</p>"));
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>figure " + chapter + (is_section || is_subsection ? "." + section : '') + (is_subsection ? "." + subsection : '') + String.fromCharCode(96 + figure) +"</a>");
                });
            }
            if ($(this).is("toc_table")) {
                var name = $(this).attr('id');
                table += 1;
                $(this).append($("<p style='text-align:center;'><b>Table " + chapter + (is_section || is_subsection ? "." + section : '') + (is_subsection ? "." + subsection : '') + String.fromCharCode(96 + table) +"</b>: " + $(this).attr('description') + "</p>"));
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>table " + chapter + (is_section || is_subsection ? "." + section : '') + (is_subsection ? "." + subsection : '') + String.fromCharCode(96 + table) +"</a>");
                });
            }
            if ($(this).is("toc_code_segment")) {
                var name = $(this).attr('id');
                code_segment += 1;
                $(this).append($("<p style='text-align:center;'><b>Code Segment " + chapter + (is_section || is_subsection ? "." + section : '') + (is_subsection ? "." + subsection : '') + String.fromCharCode(96 + code_segment) + "</b>: " + $(this).attr('description') + "</p>"));
                $("toc_reference").each((i, element) => {
                    element = $(element);
                    if (element.attr('name') == name)
                        element.html("<a href='#" + name + "'>code segment " + chapter + (is_section || is_subsection ? "." + section : '') + (is_subsection ? "." + subsection : '') + String.fromCharCode(96 + code_segment) + "</a>");
                });
            }
        });

        var real_index = 1
        $(".toc_chapter").children().each(function (index) {
            if ($(this).hasClass("page")) {
                $(this).children().append($("<div style='position: absolute; left:48%; bottom:-2cm;'><p style='text-align:center;'>" + (real_index++) + "</p></div>"));
            }
        });

        toc_page = toc_list.parent().parent();

        var main_page = toc_page.children().children();
        var children = []
        main_page.children().each(function (index) {
            children.push($(this));
            main_page.remove($(this))
        });
        children.unshift($('<h2 style="text-align:center; font-size: 20pt">Table of Contents</h2>'))
        main_page = main_page.parent().parent().parent().parent();
        main_page.children().remove()
        var last_main_page = $(document.createElement("div"))
        last_main_page.addClass("page")
        $(".table_of_contents").remove($(".table_of_contents").children())
        main_page.append(last_main_page)
        var last_sub_page = $(document.createElement("div"))
        last_sub_page.addClass("subpage")
        last_sub_page.append(subpage_header);
        var last_sub_page_content = $(document.createElement("div"))
        last_sub_page_content.addClass("subpage_content")
        last_sub_page.append(last_sub_page_content);
        last_main_page.append(last_sub_page);
        window.total_height = last_sub_page.outerHeight()
        while (children.length > 0) {
            element = children.shift()
            last_sub_page_content.append(element)
            window.total_height += element.outerHeight();
            console.log(window.total_height)
            if (pxTomm(last_sub_page_content.height()) > subpage_height_mm) {
                last_sub_page_content.remove(element)
                children.unshift(element)
                last_main_page = $(document.createElement("div"))
                last_main_page.addClass("page")
                main_page.append(last_main_page)
                last_sub_page = $(document.createElement("div"))
                last_sub_page.addClass("subpage")
                last_sub_page.append(subpage_header)
                var last_sub_page_content = $(document.createElement("div"))
                last_sub_page_content.addClass("subpage_content")
                last_sub_page.append(last_sub_page_content)
                last_main_page.append(last_sub_page);
                window.total_height = last_sub_page.outerHeight()
            }
        }
        window.bibtex_callback = update_references
        createWebPage(current_template, true)
    }
    var references_bibtex_display = null;
    var references_bibtex_display_parent = null;
    function update_references(bibtex_display) {
        $("toc_frontmatter_entry").children().each(function (index) {
            $(this).children().append($("<div style='position: absolute; left:48%; bottom:-2cm;'><p style='text-align:center;'>" + romanize(index + 1) + "</p></div>"))
        });
        $('.bibtexentry').removeClass("bibtexentry")
        bibtex_display = $(bibtex_display)
        references_bibtex_display_parent = bibtex_display.parent()
        bibtex_display.remove()
        $('.bibtex_reference').addClass("bibtex_display")
        references = true;
        $('.bibtex_template').remove()
        references_bibtex_display = bibtex_display;
        window.bibtex_callback = update_references_section
        references_bibtex_display_parent.append($('<div id="phantom_bibtex_display" class="bibtex_display" callback="window.bibtex_callback(bibtex_display)"style="text-align: left;"></div>'))
        createWebPage(`
        <a class=\"bibtex_template\"><div class="if author"><span class="author" max="1"><a href=""><span class="last"></span></a></span></div></a>`, false);
    }
    function update_references_section(bibtex_display) {
        $("#phantom_bibtex_display").remove();
        references_bibtex_display_parent.append(references_bibtex_display)
        hljs.initHighlighting();
    }
</script>